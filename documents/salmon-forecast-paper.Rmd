---
title: "Identifying Frontiers in Ecological Forecasting with Computer-Age Statistical Methods"
author:
  - Daniel Ovando
  - Curry Cunningham
  - Peter Kuriyama
  - Chris Boatright
  - Ray Hilborn
date: "`r Sys.Date()`"
output: 
  bookdown::pdf_document2: default
  bookdown::word_document2:
    reference_docx: template.docx
bibliography: "../references.bib"
csl: nature.csl
params: 
  results_name: ["v0.5.4"]
linkcolor: blue
toc: true
header-includes:
  - \usepackage{setspace}\doublespacing
  - \usepackage{lineno}\linenumbers
  - \usepackage{draftwatermark}
  - \SetWatermarkText{DRAFT}
  - \SetWatermarkScale{1}
abstract: |
  Forecasting of fish populations is an integral part of management systems where quotas are set, and are often used by the harvest and processing sectors to make business plans.  Almost all fish forecasts are currently based on population dynamics using estimates of birth rates and survival from recent years.  Computer-age statistical methods such as dynamic linear modeling, machine learning and empirical dynamical modeling have fundamentally transformed many aspects of our society by providing previously impossible levels of predictive accuracy to tasks from digital advertising to weather forecasting. However, these methods are underutilized for predicting and managing natural resources. Salmon are an ecological and economic keystone of Alaska and the forecasts of salmon runs are used to plan staffing and supplies for processors and crew size and even whether to operate for the fishing fleets. These predictions are challenging to make though, given the complex set of dynamic factors that affect numbers of returning salmon, from egg production to predation to environmental drivers.. We utilize data from the salmon populations of Bristol Bay, Alaska to evaluate the potential for computer-age statistical methods to improve the accuracy of predictions of annual returns. Salmon present an ideal case study due to the presence of long and accurate time series of annual returns, and the ecological and economical importance of reliable predictions to commercial fisheries and subsistence-dependent communities. We demonstrate how computer-age methods can help researchers and managers provide improvements in forecast skill, and identify when the bottleneck predictive skill is model design versus data. We also show that these methods may not be a panacea for improving predictive performance when confronted with time series of short duration, as are often found in analysis of population dynamics and ecological forecasting applications at annual time steps for aquatic and terrestrial species.
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,  dev = "cairo_pdf", message = FALSE, warning = FALSE)
library(tidyverse)
library(patchwork)
library(cowplot)
library(magick)
library(hrbrthemes)
library(ggplot2)
library(Cairo)
library(extrafont)
library(here)
library(scales)
extrafont::loadfonts()

results_dir <- here("results", params$results_name)

pub_theme <-
  hrbrthemes::theme_ipsum(base_size = 10, axis_text_size = 8, axis_title_size = 12) +
  theme(
    panel.spacing = unit(0.5, "lines"),
    plot.margin = unit(rep(10, 4), units = "points")
  )

theme_set(pub_theme)


# load plots

load(file = file.path(results_dir, "plots.RData"))

load(file = file.path(results_dir, "performance.RData"))

load(file = file.path(results_dir, "forecasts.RData"))


```

<!-- Summary: Rather than looking for one silver-bullet model, using multiple computer-age statistical models allows for improved forecasting and also highlights situations where you simply need more data. AND SALMON ARE COOL -->

<!-- Target Journal: Scientific Reports/ Ecological Applications / CJFAS / Nature Communications -->
<!-- Abstract: 150 words -->
<!-- Words: If we want to keep our options open let’s keep it under 5,000 words (Nature Communications is unclear whether that is 5,000 + 3000 max for methods, or whether methods section of can’t be more than 3000 of 5000) -->
<!-- Figures: 10 -->
<!-- *idea for title from this book https://web.stanford.edu/~hastie/CASI/, since in theory we’re not just exploring machine learning if we’re including EDM, but can discuss if we like it this term or not -->

<!-- Nature Communications: Introduction, Results, Discussion, Methods -->


<!-- * Definitely some similarities with the Ward, Holmes, Thorson, and Collen (2014) paper: Complexity is costly: a meta-analysis of parametric and non-parametric methods for short-term population forecasting. Theirs was squarely focussed on time series models. -->

<!-- Rather than looking for one silver-bullet model, using multiple computer-age statistical models allows for improved forecasting and also highlights situations where you simply need more data.  -->


<!-- \AddToShipoutPictureFG{ -->
<!--   \AtPageCenter{% or \AtTextCenter -->
<!--     \makebox[0pt]{\rotatebox[origin=c]{45}{% -->
<!--       \scalebox{5}{\texttransparent{0.3}{DRAFT}}% -->
<!--     }} -->
<!--   } -->
<!-- } -->



# Introduction

<!-- These things have revolutionized lots of fields, but not so much ours yet. Use this to show how it could be done.  -->

Animal populations exhibit complex dynamics driven by interactions with a myriad of aspects of their ecosystem. Predicting the outcomes of these dynamics is a critical task of natural resource management.  Forecasts of future abundance are often used to set fisheries quotas,  and vessel operators may make decisions about alternative fisheries based on predicted abundance.  Processors make long term and short term investment plans in staffing and capacity based upon forecasts. However the methods used to make these predictions rely on methods that are decades old and generally do not go beyond forward projection from age structured models.


The past two decades have seen explosive progress in the ability of “computer age” statistical methods to make accurate predictions [@efron2016], revolutionizing fields such as financial modeling, weather forecasting, and medicine. However, these methods remain relatively unused in ecological forecasting, particularly in an applied setting [@peters2014]. We use the ecologically and economically vital sockeye salmon populations of Bristol Bay, Alaska, to demonstrate both the potential and limitations of computer age statistical methods in ecological forecasting, by directly comparing the realized predictive performance of a wide range of applicable statistical forecasting tools.

The commercial salmon fishery in Bristol Bay, Alaska is the single largest sockeye salmon (*Oncorhynchus nerka*) fishery in the world [@steiner2011], with an estimated wholesale value of the Bristol Bay commercial sockeye harvest was $390 million USD in 2010 and representing approximately one-sixth of the total value of all US seafood exports [@knapp2013]. Salmon returning to Bristol Bay also provide vital food security for subsistence-dependent Alaskan communities, and critical vectors of marine-derived nutrients that support vibrant freshwater habitats [@naiman2002; @schindler2003]. Sustainable management of the Bristol Bay salmon fishery is dependent upon accuracy of preseason forecasts for salmon abundance, which inform development and implementation of inseason harvest strategies and successful prosecution of subsistence and commercial fisheries. Preseason forecasts are also important for planning by the processing industry as a basis for identifying the appropriate level of supplies, equipment, and personnel necessary to process the annual harvest, and directly influence the profitability and efficiency of the salmon industry as a whole. 

Sockeye salmon are born in rivers, where they spend the first one or more years of their lives. Eventually, these fish transition to the oceans, where they live the remainder of their lives until they return to their natal rivers to spawn and then die. Sockeye salmon can spend a range of years in these freshwater and oceanic phases, denoted as age groups. For example, a fish in the 1_2 age group spent one post-birth year in the river system, and 2 years in the oceans. The Bristol Bay sockeye salmon fishery is primarily made up salmon from seven different river systems, each of which is managed as a separate stock Fig.\@ref(fig:returns). 


```{r returns, fig.cap = "Annual total (A), by river system (B), and by age group (C) returns of sockeye salmon (*Oncorhynchus nerka*) to Bristol Bay, Alaska. Age group is formatted by 'years spent in freshwater'_'years spent in ocean'. Map adapted from @cunningham2019", fig.width=7, fig.asp=.8}
return_plot
```
  
Why might we expect computer-age methods to be well-suited to the task of salmon forecasting? Methods such as machine learning excel at identifying and exploiting complex interactions between variables in a system. Conversely, “traditional” statistical methods often restrict themselves to simplified linear and often non-dynamic representations of natural systems, both for analytical tractability and to facilitate understanding of underlying processes. Typically, these traditional statistical methods are concerned with explicitly estimating and interpreting model parameters rather than solely forecast population size [@beyan2020; @malde2020]. We would expect computer-age methods to show substantial improvement in predictive power then when the true underlying system linking observed variables and outcomes differs dramatically from more simplified representations of the system used by conventional statistical approaches. In the case of salmon, we know that run sizes are affected by a wide range of ecosystem variables, including spawning success, river conditions, oceanic predator and prey abundance, and competition with other salmonids [@connors2020]. 

```{r}
fri_system_mape <- system_forecast %>% 
  filter(model == "fri") %>%
  group_by(model) %>% 
  summarise(mape = yardstick::mape_vec(observed, forecast))

fri_age_mape <- age_forecast %>% 
  filter(model == "fri") %>%
  group_by(model, age_group) %>% 
  summarise(mape = yardstick::mape_vec(observed, forecast))

# system_forecast %>% 
#   ggplot(aes(observed, forecast)) + 
#   geom_point() + 
#   geom_abline(aes(slope = 1, intercept = 0)) +
#   facet_wrap(~model)

```


Rather than incorporate these factors directly, salmon forecasting has traditionally relied on cohort or “sibling” regression methods, in which returns of for example four year old fish are predicted by the returns of three year old fish observed in the previous season. There are good reasons for this practice: trends in sibling abundance internalize many factors affecting salmon returns. If a particular cohort suffers from poor environmental conditions and large amount of competition with other predators, the impacts of that will be reflected in the return abundance of younger age classes from the same cohort (i.e. originating from the same brood year) that experienced similar environmental conditions and resource availability, and by extension survival, as juveniles. However, this method does have shortcomings, most notably the underlying assumption of consistency in the relationship between the abundance of age classes and stability in the maturation schedule (i.e. the probability of salmon maturing and returning to freshwater to spawn after a given number of years in the ocean). For example, if environmental conditions cause members of a cohort of salmon to spend more time at sea than in previous years, a sibling regression might under predict the number of future returns. In addition, sibling regression requires accurate observations of the return abundance for younger sibling age classes,  limiting the ability of these models to perform as well in predicting returns of younger salmon for which few or no siblings have been observed.

We hypothesize that directly incorporating data on potential influencers of salmon return size, rather than relying on sibling returns alone, might help improve forecasting given the complex dynamics of salmon populations. However, these variables are likely to have complex, non-linear, and non-stationary effects on  salmon populations,  potentially obscuring their value from conventional statistical approaches and manually designed models. In order to explore this possibility, we use a suite of `r n_distinct(system_performance$model) - 2` methods together with a panel of data on salmon populations and environmental conditions in Bristol Bay, Alaska to explore what if any improvements in forecast skill can be achieved. These models include two machine learning methods (a random forest [@breiman2001; @wright2017] and a boosted regression tree [@chen2020]), empirical dynamic models [@munch2020 and references therein], and dynamic linear models [@petris2009; @pole1994]. We also include two benchmark models; a lag-1 model in which the predicted returns for a given age group and river system in a year are equal to the observed returns for that that age group in that system in the prior year, and a running mean model in which the predicted returns are a 5 year running mean of the observed historic returns.  We find that while these methods can make meaningful improvements in some cases, no one model stands out as a clear winner, highlighting the need for multi-model inference in ecological forecasting. Beyond improving predictions, multi-model inference can separate bottlenecks in forecast skill are due to model performance or data limitations. 


<!-- Introduction notes -->
<!-- First synthetic review of multiple methods and ensembles for this kind of popdy processes. Answering question of is it data or is it models.  -->


<!-- Ensemble models often presented as a solution to improved forecasting. But, we show here they also serve a secondary and overlooked purpose: separting out poor predictive performance as a function of model misspecification vs. data availability -->


<!-- The strength of these is not really in like doing a better job with univarite time series analysis  its in being able to exploit relationships between multiple datasources that are all biological linked. Broaden the net. Absence of other data, a lag is just fine, as opposed to kind of just throwing the kitchen sink of variables without some kind of subject expertise. Bridgin gap between kitchen sink and just traditional approach. Fruitful middle  -->


<!-- Point out that ICES special issues dedicated to ML in marine science, with not one article looking at the popdy question specifically  -->


<!-- Central tension to highlight: researchers are faced with uncertainty of whether predictive skill is driven by model mispecification or lack of data. This approach can help, focus in on Peter’s point about years places when all models fail as indicator  -->


# Results


We judge the performance of all of our models by scaled root mean squared error (SRMSE). RMSE is calculated as 

$$ RMSE_m = \sqrt{\frac{1}{I}\sum\limits_{i = 1}^I(y_{i} - f_{i,m})^2} $$

where *i* represents an observation of numbers of returning salmon *y*  and the forecast for those numbers *f* by a given model *m*. In the manner of mean absolute scaled error (MASE, @hyndman2006), we scale each models RMSE for the a given resolution by the RMSE of a lag-1 model for the same resolution (for example at the river system level). 


$$ RMSE_{l1} = \sqrt{\frac{1}{I}\sum\limits_{i = 1}^I(y_{i} - y_{i,l1})^2} $$


and SRMSE for model *m* is then 


$$SRMSE_m = \frac{RMSE_m}{RMSE_{l1}}$$

MASE is commonly used to judge the accuracy of predictions derived from time series models, since it compares the error of a given model to the error expected by a simple model in which the predictions in a given time step are equal to the observed values in the last time step (an lag1 model). We use RMSE instead of MAE to reflect the use of the forecast. MAE considers an error of 10 to be twice as bad as an error of 5. However, in the context of salmon forecasting, our primary objective is to avoid  massively over or estimating the pre-season forecast. RMSE penalizes large errors more than small errors, helping select models that avoid the kinds of large errors that are most problematic for the task of managing salmon populations. 

A SRMSE of 1 means that a model has predictive performance equal to that of a lag-1 model. A SRMSE greater than one indicates that a given model performs worse than the lag-1 benchmark, a SRMSE less than one that a model performs better than the lag-1 benchmark [@hyndman2006]. Forecasts in a given year are produced by a model trained on data on all preceding years.

<!-- XX Need a brief table of each of the methods describing them and their acronyms in the intro or start of the results XX -->

```{r models}

sys_performers <- system_performance %>% 
  filter(!model %in% c("fri", "boost_tree_ensemble")) %>% 
  group_by(system) %>% 
  filter(srmse == min(srmse))

age_performers <- age_performance %>% 
  filter(!model %in% c("fri", "boost_tree_ensemble")) %>% 
  group_by(age_group) %>% 
  filter(srmse == min(srmse))

```

Management of Bristol Bay sockeye salmon is organized at the river-system level. As such forecasts at this resolution are critical for setting management and harvest plans. For each river system, we selected the model with the lowest SRMSE over the years 2000 to 2019 as the model of choice for that system. Systems varied in both the lowest SRMSE achieved and in the model that produced the best performance. At least one model was able to beat a simple lag-1 benchmark model in each of the systems, with the `r  sys_performers$model[sys_performers$srmse == min(sys_performers$srmse)]` model achieving a SRMSE of `r prettyNum(min(sys_performers$srmse), digits = 2)` at the top end in the `r sys_performers$system[sys_performers$srmse == min(sys_performers$srmse)]` system, and the `r sys_performers$model[sys_performers$srmse == max(sys_performers$srmse)]` model on providing only marginal improvement over a lag-1 model with a SRMSE of `r prettyNum(max(sys_performers$srmse), digits = 2)` in the `r sys_performers$system[sys_performers$srmse == max(sys_performers$srmse)]` river system. Of all the candidate models explored, only the DLM, boosted regression tree, and random forest models were selected as the best performer in at least one system (Fig.\@ref(fig:sys-forecast)). 
  

```{r sys-forecast, fig.cap="Observed (grey ribbons) and predicted (colored points) returns of sockeye salmon to top seven systems by total sockeye salmon returns in Bristol Bay, Alaska. Color corresponds to the best performing model in terms of scaled root mean squared error (SRMSE), transparency reflects the SRMSE of the best performing model."}
system_forecast_figure

```

```{r}

mean_srmse_delta <- system_performance %>% 
    filter(!model %in% c("fri", "boost_tree_ensemble")) %>% 
  group_by(system) %>% 
  mutate(rmse_rank = rank(rmse)) %>% 
  filter(rmse_rank <= 2) %>% 
  arrange(system) %>% 
  group_by(system) %>% 
  summarise(delta  = srmse[rmse_rank == 2] - srmse[rmse_rank == 1]) %>% 
  ungroup() %>% 
  summarise(mmd = mean(delta))

```


The clearest outcomes from our results at the system level is that no one model was dominant, and systems varied greatly in their predictability. Why would it be that some models would outperform others at the river system level? One explanation is that simply, given a set of roughly equally effective models, and a small sample size per system (n = 19 years), we might expect different models to perform best in different systems by chance alone. The mean difference in SRMSE between the best and second best models by system was only `r prettyNum(mean_srmse_delta$mmd, digits = 1)`, suggesting that generally several models are capable of outperforming a lag_1 model with roughly equal skill. 

Another explanation could relate to the age group makeup of runs at in different river systems. While it is not intuitive as to why one model would perform better than another in a given river system all else being equal, we might expect different models to perform better for different age classes. The dynamic linear model is at its core still a sibling regression, and as such we would expect it to perform better on older age groups. Conversely, while the machine learning and EDM models can use sibling returns, they are more flexible in determining which data are most useful for prediction. As such, predictions for younger age classes can be based more on environmental signals if called for by the data. We find that the boosted regression tree model, that allows for environmental and interspecies covaraites effects,  performed  best for the youngest included age group (1_2 fish) (Fig.\@ref(fig:age-forecast)). Turning back to the systems, returns to Naknek, where boost_tree performed best, have recently had large proportions of their runs made up of 1_2 fish (the group that boost_tree performed best on) (Fig.\@ref(fig:age-returns)). However, the Wood river system has also seen a massive increase in 1_2 fish, and no model was able to correctly capture the recent returns to that system. 


```{r age-forecast, fig.cap="Observed (grey ribbons) and predicted (colored points) returns of sockeye salmon within each age group returning to Bristol Bay, Alaska. Age group refers to “years spent in freshwater”_”years spent in ocean”. Color corresponds to the best performing model in terms of scaled root mean squared error (SRMSE), transparency reflects the SRMSE of the best performing model."}
age_forecast_figure
```


```{r age-returns, fig.cap = "Observed returns of sockeye salmon by age group (color) and system (panels) over time"}
age_system_return_plot
```


That some models outperform others in some circumstances (e.g. particular systems or age classes) suggest a simple solution: pick the best model for the resolution of prediction required (e.g. the system level), or perhaps better yet create an ensemble model that can leverage the relative strengths of individual models into one improved forecast [@anderson2017a; @araujo2007; @Dietterich2000]. We consider two such ensembles here. Each ensemble is constructed by judging the performance of different models in the past and creating a prediction for the current time step based on the historic performance of component models (the ensemble members). We constructed an ensemble model using boosted regression trees, predicting the total returns by system as a function of the predictions by system and age group of each candidate model. 

The second ensemble model is the historic published FRI forecast. The FRI is a manually constructed ensemble model, which has used various methods throughout the years to arrive at pre-season forecasts for each system based on the recent performance of different candidate models. The FRI forecast for a specific stock by age class combination was traditional constructed by AIC-weighting across candidate linear models of predicting the target age class with 1 or 2 younger age classes as predictors, and selecting the candidate predictive models fitting to two alternative time series (1963+ and 1980+ to accounting for broad-scale shifts in average Bristol Bay salmon population productivity after the shift in the Pacific Decadal Oscillation (PDO) in the late 1970’s) and two data conditions (natural or log-transformed). However, since 2013 the FRI forecast ensemble has been constructed by comparing the performance of the traditional linear and log-linear AIC-weighted sibling models, random forest models, dynamic linear sibling models, boosted regression trees, and simple autoregressive integrated moving average (ARIMA) time series models, and selecting the model with the lowest residual error in predictions for the target stock-age group across a 20-year time horizon.

We selected the top performing (in terms of SRMSE) ensemble model for each of the main river systems, and compared the performance of that ensemble to the best performing non-ensemble model in that system. One of the two ensemble methods provide improved forecasts for two out of the seven systems included here, and in four of the seven systems the boosted regression tree method produced the preferred ensemble (the FRI forecasts being preferable of the three ensembles in the other two systems). In many of the systems though, one of the individual models performed better than the ensemble (Fig.\@ref(fig:ensemble-forecast)). 

```{r ensemble-forecast, fig.cap="Performance of candidate ensemble models. Shape (diamond, circle) indicates which ensemble had the lowest SRMSE. Fti refers to the published forecasts by the Fisheries Research Institute. Boost tree ensemble is an ensemble model constructed by a boosted regression tree. For every system picked best performing ensemble, color coded by percent improvement over the best non-ensemble. Blue colors indicate that the ensemble performed better than the best non-ensemble model, red colors that the best non-ensemble model performed better than the best ensemble model"}
system_ensemble_forecast_figure
```

The underlying belief of such an ensemble strategy is that the information needed for a good forecast is present in the data, and the key is finding the model that is best able to find that information. However, no model can find information that simply is not there, or succeed if it is based upon data that is subject to overwhelming observation error. Examining trends in the annual residuals by model and river system shows clear patterns. In some years and systems, all models perform similarly well, indicating that the information needed for a good forecast is present and detectable by each of the models (e.g. Nushagak before 2015). In other years, only particular models performed well, while others struggled, indicating that information needed for a robust forecast was present but only some models were able to accurately quantify the underlying relationship, highlighting the value of ensemble methods (e.g. Naknek between 2005 and 2010). However, in other years and systems all models struggled, for example the Wood River in 2018 and the Kvichak River in 2014. This provides evidence that the information needed to perform a good forecast in those years was simply not present in the data that are currently available Fig.\@ref(fig:resids). 


```{r resids, fig.cap="Centered and scaled annual residuals (forecast minus observed) by river system and model over time. Red bands indicate areas more than one standard deviation from the mean residuals. Years in which all the lines are within a red band indicate periods where all the models struggled to provide reasonable forecasts."}
yearly_system_resid_struggles_figure
```


<!-- Which variables were most important in our forecasts? XX not sure if /how this can be done for the other models, or if it’s worth it XX. Maybe just do variable importance plots by age group for the tree based methods -->


<!-- * Performance in total returns forecast -->
<!-- * Break down by combinations of age class / system -->
<!-- * Variable importance where available -->
<!-- * Synthesize top performs for different objectives  -->
<!-- * Result of simple ensemble -->
<!-- * Result of models predicting boom / bust years  -->

# Discussion

```{r}

sys_improv <- system_performance %>% 
  select(model, system, srmse) %>% 
  pivot_wider(names_from = model, values_from = srmse) %>% 
  pivot_longer(-c(system, fri), names_to = "model", values_to = "srmse") %>% 
  group_by(system) %>% 
  filter(srmse == min(srmse)) %>% 
  ungroup() %>% 
  summarise(srmse_improv = mean((srmse - fri) / fri))

age_improv <- age_performance %>% 
  select(model, age_group, srmse) %>% 
  pivot_wider(names_from = model, values_from = srmse) %>% 
  pivot_longer(-c(age_group, fri), names_to = "model", values_to = "srmse") %>% 
  group_by(age_group) %>% 
  filter(srmse == min(srmse)) %>% 
  ungroup() %>% 
  summarise(srmse_improv = mean((srmse - fri) / fri))

```


The annual return of sockeye salmon to the river systems of Bristol Bay Alaska is an economic and ecological marvel, providing critical nutrients and value to the Alaskan ecosystems, economy, and people. Effective management of this critical natural and cultural resource depends in part on the ability to accurately forecast the number of salmon that will return each year. As such, accurate and precise forecasts for future salmon abundance are critically important to the biological and economic sustainability of this important fishery. Computer-age statistical methods can improve our forecast ability, substantially in some cases. Individual models used here improved age group specific forecasts on average by `r percent(-age_improv$srmse_improv)`, and `r percent(-sys_improv$srmse_improv)` at the system level. Ensembles of these individual methods, both formal and informal in nature, allow for further improvement in some cases. 

However, our residual analysis suggests that in some instances we simply need to include different data in the model to if we hope to improve forecasts. For example, none of our models were able to predict the massive spike in returns of the 1_2 age class to the Wood River system in recent years (Fig.\@ref(fig:resids)). We can use the results of our most recent estimated boosted regression tree model to examine the relative importance of different included data streams in forecast skill. While these importance scores cannot be interpreted in the same manner as regression coefficients, they give us a sense of where we might look for new data. Across all systems, prior returns in that system were an important predictor (and in many systems past returns in other systems were also a useful predictor). Along with these we found that both oceanographic variables, mean sea surface temperature throughout the range of the oceanic phase of the salmon during the span of their time in the ocean and sea-surface air pressure during their time in the ocean were in some systems important predictors. In addition, as reported by @connors2020, we find that in some instances abundance of other salmon species (chum salmon, *Oncorhynchus keta*, in western Kamchatka northern British Columbia , pink salmon, *Oncorhynchus gorbuscha*, in Prince William sound) Fig.\@ref(fig:imp-plot). These results suggest that improved data on at-sea conditions and interspecies competitors may facilitate improved forecasts in the future.


```{r imp-plot, eval = TRUE, fig.cap="Mean variable importance across systems of variables with importance greater than 0.05."}
system_varimportance_figure
```

Ward et al. [@ward2014] also explored the use of similar models to those used here in forecasting natural populations. They however found that the sorts of computer-age methods explored here generally performed worse than simple autoregressive models, while being substantially more computationally intensive. In contrast we find that our lag and moving average models are dominated by the more complex computer-age models across every resolution we evaluate. What might explain this discrepency? Ward et al. [@ward2014] constrained themselves to making predictions of future population size solely based on historic population size. In this way their methods are most similar to the EDM models explored here, which provided the least improvement over our lagged benchmark models. We would only expect computer-age methods to provide an improvement over simpler methods only if there are substantial complexities in the relationship between past and future abundance that simpler models miss (i.e. the simper models are misspecified). The results of Ward et al. @ward2014 and the performance of the EDM models tested here relative to our benchmark models suggest simpler models may indeed be preferable when only historic timeseries are available as a data source.  

In contrast though the DLM and machine learning methods both have access to additional information that may account for their ability to outperform simpler models. The DLM model, while still relying solely on historic return data, provides structure that autoregressive style models might miss. In particular, the DLM model uses our ability to track cohorts over time and use these data as predictors in a model. The machine learning methods explored here have access to much more data than the historic returns alone, including environmental conditions and abundance of other salmonids. In addition, the machine learning methods used here are able to leverage correlations in returns across multiple age groups and river systems (Fig.\@ref(fig:imp-plot)). While we have access to over 50 years of data, longer than some of the series reported in Ward et al. [@ward2014], our sample sizes are still minute compared to the sample sizes in most applications of computer-age statistical methods, indicating that these methods can still be used with the kinds of sample sizes encountered in population dynamics. The results of Ward et al. [@ward2014] suggest that more complex models may provide little benefit when the only data available are historic trends in the population being forecasted. We find though that computer-age methods can provide meaningful improvements when given additional covariates and/or structure. 

<!-- What other data should we suggest? We don’t include any in-river condition data, do we just not have it? Thoughts on other places we can invest? [g] -->

The field of ecology is generally concerned with developing theories and evidence for why ecosystems are structured and behave the ways they do. Methods such as those described here may not provide this sort of insight. They are instead instruments designed for prediction, often at the expense of interpretability. However, in the case of natural resources management that often depends on making decisions today based on predictions about the future, methods such as those presented here can present substantial opportunity. In particular, leveraging multiple different modeling approaches can help uncover otherwise hidden information in available data. As we demonstrate here though, we cannot simply rely on computer-age statistical methods to automatically solve problems of prediction in ecology. Even when statistical methods are unable to provide accurate forecasts though, using multiple computer-age methods as presented here can help identify when these prediction failures are due to model misspecification, and when they are do to a simple lack of sufficiently informative data. Computer-age statistical methods then can both improve forecasts and help identify frontiers in predictability in ecological systems given currently available data. Once these borders are found, we can focus efforts and resources on collecting new data that can push our predictive models to greater heights. It is critical that we allocate resources to both the advancement of predictive modeling methods, and to the hard work of collecting the data from the natural world that are the foundation of any successful forecasting efforts. 


<!-- * Make clear that the point here is that some clever person that’s better at ML than we are might not be able to get slightly better performance in some cases: but that all models fail in similar ways in similar situations suggest that there are only marginal gains to be made in further model tinkering -->
<!-- * Contrast to Ward, Holmes, Thorson, and Collen (2014): basically subtly say that the issue there was trying to use some of these methods “out of the box” i think? -->
<!-- * Discuss potential reasons why different models performed better for different circumstances -->
<!--    * ML for young -->
<!--    * Structural for older -->
<!-- * Put forecast improvements (where they occured) into economic /ecological context: how much is a XX% increase in forecast accuracy worth? -->
<!-- * In the specific case of salmon forecasting, results suggest that we have gone about as far as we can in terms of pre-season forecasting given the available data. Obtaining higher quality forecasts in the future then will require investment in data collection -->
<!--    * Use models to suggest that where that data collection might be targeted -->
<!-- * How much data was needed to make salmon forecasts accurate -->
<!--    * Demonstrate retrospective bias plots showing that ML methods actually perform pretty well 20 years out of sample - you don’t need millions of data points to access these things -->
<!--    * Out of sample performance as a function of amount of training data -->
<!-- * Computer-age methods can help identify “ceilings” in predictability given available data -->
<!--    * Incredibly useful for ecological prediction in general: helps people get a sense for when they might be able to squeeze more insight out of the same data, and when they need to go out and just collect more data -->
<!-- * But also clear role and importance for “traditional” methods and not putting all eggs in one basket -->
<!-- * Fundamental challenge of time series length (n=years) -->
<!--    * ML and EDM methods designed to be conditioned on many more system observations -->
<!-- * Axes across which to compare models -->
<!--    * Predictive capacity (bias/precision) -->
<!--    * Implementation costs (i.e. tuning time, ect) -->
<!--    * Interpretability (i.e. level of black-boxicity, nature of uncertainty) -->
<!--    * Risk of extreme poor year (1/10 years model predicts an extremely unbelievable/unlikely run size) -->
<!-- * Closing statements -->
<!--    * These sorts of methods can be particularly valuable and risky under a changing climate (valuable if they capture changes, risky if they get stuck on the past) -->
<!--    * Salmon are an ideal case study in how to consider these tools given the richness of data and ability to vet results -->
<!--    * These methods may help improve forecasts in a changing climate, but only to a point: eventually you need to collect more data -->


# Methods (2000 words)
<!-- * Applicable details of current FRI forecast methods -->
<!-- * Applicable details of DLM methods -->
<!-- * Applicable details of candidate ML methods -->
<!-- * Applicable details of candidate EDM methods -->
<!-- * Validation procedure (leave-one-out testing from 2000-2018) -->


All code and data needed to fully replicate our results are publicly available at https://github.com/DanOvando/salmon-forecast-paper/. We describe critical details of each our main methods here.

All models were compared by one-step-ahead forecast skill, defined here by SRMSE. Each of the evaluated models generate forecasts at the resolution of age group and river system in a given year. Forecasts for a given year are produced by a model trained on all years after 1963 and prior to the year for which a forecast is desired. This is performed in a rolling fashion, such that for example forecasts for the year 2018 is produced by a model trained on data from 1963 to 2017, the 2019 forecast by a model trained on data from 1963 to 2017, and so on. Each method has its own ways of tuning and validating the model, but all such steps are performed only the training data: all data for the forecast year are quarantined from all aspects of all model fitting until the final results are produced. 

## Machine Learning Models

<!-- [I think machine learning used a kitchen sink approach where you took all the available data and fed it through the models? That’s the reason why you get the variable importance plots shown in the discussion] -->


We evaluate three different machine learning models: a random forest (implemented through the `ranger` package [@wright2017] in R [@rcoreteam2020]), boosted regression trees through the `xgboost` package [@chen2020]. A recurrent neural network implemented through `tensorflow` [allaire2020] through the `keras` interface [allaire2020a] was also tested but was found to perform poorly relative to the other methods and to be extremely computationally intensive, and as such was not included in the main analysis. Random forests are ensembles of regression trees, which make predictions by selecting nested splits of variables and mapping the mean level of the dependent variable at the terminal nodes of each tree. Boosted regression trees are similar to random forests, but have mechanisms in place that actively update the model to address data points that the model is struggling to fit. For all machine learning methods, within a model fit the model selects splits/transformations/coefficients in order to minimize some objective function on held-out data (e.g. minimize the RMSE of predictions on data held out from the fitting process by the algorithm). 

Each of the machine learning methods had access to a variety of population and environmental data. We fit versions of each model separately for each age group in each major river system. Within that system, the candidate predictor variables included the sibling returns for the cohort in question in all river systems, the numbers of the cohort that spawned the cohort in question, along with the cumulative mean environmental conditions (sea surface temperature, pacific decadal oscillation, sea surface air pressure, and degree of upwelling) over the years the cohort spent at sea, and the reported numbers of chum and pink salmon throughout the North Pacific reported by @ruggerone2018. 

Models were fit at the level of age groups by river system. Data were first split into training and testing sets. We then split each of the training sets (e.g. all data before the year 2000 if the year 2001 is to be forecasted) into a series of analysis and assessment splits. Given the timeseries nature of the data, we generate these analysis and assessment splits in a rolling manner: for the first split, we use the first 70% of the training year to fit a model, and 30% to evaluate the performance of that model, followed by the first 75% and 25%, etc. These analysis and assessment splits are used to tune nuisance parameters common to all machine learning models, for example the minimum node size of fitted trees. We fit each of our assessment splits across a grid of potential parameter values, and select the set of tuning parameters that minimize the root-mean squared error (RMSE) of the predictions on the assessment splits. 

Once the set of tuning parameters for each training set were selected, we then fit the final model using all the training data with those tuned parameters, and then used that model to predict the returns in the testing set.  All relevant data transformations were prepared only on training / analysis splits (e.g. means and standard deviations for centering and scaling) and then applied to testing / assessment splits. 

## Dynamic Linear Models

Traditional methods for forecasting sockeye salmon abundance in Bristol Bay and throughout Alaska, have relied on the relationship between the abundance of different age classes from the same cohort, or originating from the same brood year, but returning to breed in subsequent years. These “sibling” or cohort regression models leverage the fact that salmon spawned in the same brood year and migrating to sea in the same year exhibit similar patterns in survival, based upon their shared exposure to the same physical environmental conditions, prey resource availability, and predator field, as juveniles and at ocean entry.
 
Foundational to the predictability of sibling relationships is the assumption that the ratio of returns by age class remains stable across time. In a context of a linear model, for example, we can model returns as $\hat{R^{1_3}_t} = \alpha + \beta{R^{1_2}_{t-1}}$  where $\hat{R^{1_3}_t}$ is the predicted return abundance of the older age class and  $R^{1_2}_{t-1}$ is the observed abundance of the the same cohort in the prior time step. Under a classic sibling regression the assumption is that the estimated  parameters $\alpha$ and $\beta$ remain constant across time. However, there are multiple conditions under which both the average return abundance of a particular age class or the ratio of abundances among age classes might change over time. For example, if the average maturation schedule (e.g. the probability that an individual will mature after 2 vs. 3 years in the ocean) changes in response to natural or anthropogenic selection, the assumption of a stationary  parameter is violated. Alternatively, if average marine mortality experienced by salmon changes as a result of large-scale climate, ecosystem, or trophic shifts, this should be reflected by changes in both  and  parameters of the sibling relationship.
 
To better represent the dynamic nature of sibling or cohort relationships over time and improve predictive performance, we implement dynamic linear models (hereafter DLMs). DLMs are a class of regression models where the values of regression coefficients are permitted to evolve over time, rather than remain static [@petris2009; @pole1994]. DLMs were fit to available data using a single predictor age class (one fewer year in the ocean, returning the prior year), and allowing for evolution of both the slope and intercept parameters over time, as: 


$$\hat{R^{1_3}_t} = \alpha_t + \beta_t{R^{1_2}_{t-1}} + \epsilon_t$$

Both regression parameters are described by a random walk (i.e. $\alpha_t \sim Normal(\alpha_{t-1},\sigma^2_{\alpha})$ and $\beta_t \sim Normal(\beta_{t-1},\sigma^2_{\beta})$), and errors were assumed normally-distributed ($\epsilon_t \sim Normal(0,\sigma^2_{\epsilon})$). DLMs were implemented using the Multivariate Autoregressive State-Space Modelling (MARSS) package (version 3.10.12) in R [@holmes2012; @holmes2020].


## Empirical Dynamic Modeling

Empirical dynamic modeling (EDM) is a nonparametric approach to characterize ecological dynamics and generate forecasts. The approach is predicated on Takens’ theorem, which states that a single time series and a number of lags (dimension; E) are representative of overall system dynamics [@sugihara1990; @takens1981]. Applications of EDM have identified causal relationships in ecological systems [@sugihara2012] and improved forecast skill in Fraser River sockeye salmon [@ye2015]. See @munch2020 and [@chang2017] for more general overviews of EDM. We used the software package rEDM [@ye2020] for analysis.

Here, we applied simplex projection (hereafter simplex) and sequentially locally weighted global linear maps (s-maps) to the Bristol Bay sockeye data. Both methods require identifying the dimensionality (E) of a time series and constructing an attractor (a time series and its E-lagged coordinates). Leave-one-out prediction identifies the best E of a time series. We used E values ranging from 1 to 10, found the E-nearest neighbors (based on Euclidean distance) from the observation of interest, and calculated a predicted value by averaging the E-nearest neighbors. The best E had the highest correlation between observed and predicted values. S-maps is an extension of simplex that has the addition of a weighting parameter (theta) which modifies the strength of nearest neighbor weighting (theta=0 weights nearest neighbors equally; theta>0 stronger weighting of nearest neighbors) [@sugihara1994].

We used s-maps for each of the river, age-class combinations. For example, we predicted Kvichak 1_2 returns. We identified the best E for Kvichak 1_2 and also included information from the the top 6 age classes (based on average returns) for Kvichak. The attractors had a time series and its lagged coordinates and the unlagged coordinates for abundant age classes. The s-map theta values were selected based on the conditioning data set (e.g. time series up to 2000) to predict one year into the future (2001). 


## Model Comparisions

Predictive performance of candidate models was calculated by generating 1-year ahead forecasts for each target river system by age class combination, as a rolling window from the year 2000 to 2019. This method for quantifying forecast performance is most applicable to the context of this ecological forecasting problem as each candidate model is trained on data up to the year prior to the prediction year, or the data that would have been available to the analyst in each year historically. 

Even though each model predicts at the resolution of age group and river system, we generally compare model performance at coarser resolutions (for example river system across all age groups). In those cases, we first aggregate the total returns at the resolution in question (e.g. sum all observed and forecast returns across all age groups for a given system), and then the SRMSE was calculated on those aggregated data. 

We do not conduct formal statistical tests of model performance. Conventional statistical models might be compared via AIC or likelihood ratios. However, neither the machine learning or the empirical dynamic modeling methods have formal estimates of uncertainty or likelihoods, and as such cannot be compared by such methods. As such we judge model performance by the point estimates of SRMSE produced by each model. 

## Ensemble Model

The FRI ensemble forecast values were pulled from the historic pre-season forecasts. For the boosted regression tree ensemble, we follow a similar routine to that employed for the individual boosted regression tree model. We compiled the pre-season forecasts by river system and age group for each of the candidate models going back to 1991. The ensemble sought to predict the observed total returns by system using the returns by system and age class produced by each of the candidate models. For the years 2000 to 2019, we performed a series rolling model fits, where data before the testing year was held out for training (and analysis and assessment splitting and model tuning), and then used fit the ensemble model, which was then tested on the testing year. The held-out one-year-ahead predictions of the ensemble model in each time step were then compiled to create the historic series of ensemble forecasts at the river system level. 

<!-- ## Benchmark Models -->


<!-- We include two simple benchmark models as well as our computer-age models. These are a simple lag model, where the predicted forecast (numbers by age group and river system) in a given year are simply the numbers by age group and river system observed in the previous years. We also include a running mean model, in which the predicted numbers by age group and river system in a given year are the mean of the the numbers by age group and river system observed over the previous four years.  -->




<!-- [a]We might also mention that these require both knowledge of the total return by stock (river system) and age composition information to allocate returns back to the correct cohort. These are more data requirements than reasons one could not develop a reliable estimate, and might be too much salmon-specific info. -->
<!-- [b]Make this clearer. Basically saying that ML methods and the covariates they include allow for improvements when fish are young, and have their lives dominated by ocean phase (1_3). Both 2_2 and 2_3 are either older (2_2) and or spend more time in freshwater where we don't have environmental covariates in the model (2_3) -->
<!-- [c]For every system picked best performing ensemble, color coded by % improvement over next best non-ensemble, Red indicates non-ensemble model was better. -->
<!-- [d]Highlight years where there is a consistent difference, unavoidable residuals independent of method -->
<!-- [e]that recent paper that looked at effects of iterspecies competition -->
<!-- [f]add in other ruggerone paper looking at pink as a predictor of bristol bay - predeicting variance -->
<!-- [g]The best predictor to include would be juvenile (smolt) abundance at outmigration, however this has been notoriously difficult to obtain. The early smolt abundance estimates had huge CVs and the more recent and reliable estimates from verical-looking sonar only cover a short portion of the time series. -->


<!-- Other candidate predictors to describe: -->
<!-- - Offshore abundance estimates in fall/winter of outmigration (NOAA BASIS surveys) -->
<!-- - Sockeye size/condition in the brood year. We can tie this in to the maternal effects literature. Big Old Fat Females -->
<!-- - -->
<!-- [h]I think we should be explicit at the beginning of each section which data are used in the model fitting -->


# References
