---
output: 
  bookdown::word_document2:
    reference_docx: template.docx
  bookdown::pdf_document2:
    keep_tex: true
    number_sections: true
bibliography: "../references.bib"
csl: canadian-journal-of-fisheries-and-aquatic-sciences.csl
params: 
  results_name: ["v1.1.0"]
linkcolor: blue
toc: false
header-includes:
  # - \RequirePackage{totcount,xpatch}
  # - \usepackage{totcount}
  - \usepackage{setspace}\doublespacing
  - \usepackage{lineno}\linenumbers
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,  dev = "cairo_pdf", message = FALSE, warning = FALSE)
library(tidyverse)
library(patchwork)
library(cowplot)
library(magick)
library(hrbrthemes)
library(ggplot2)
library(Cairo)
library(extrafont)
library(here)
library(scales)
extrafont::loadfonts()

results_dir <- here("results", params$results_name)

pub_theme <-
  hrbrthemes::theme_ipsum(base_size = 10, axis_text_size = 8, axis_title_size = 12) +
  theme(
    panel.spacing = unit(0.5, "lines"),
    plot.margin = unit(rep(10, 4), units = "points")
  )

theme_set(pub_theme)


# load plots

load(file = file.path(results_dir, "plots.RData"))

load(file = file.path(results_dir, "performance.RData"))

load(file = file.path(results_dir, "forecasts.RData"))

 system_importance_table <-  read_rds(file.path(results_dir,"system_importance_table.rds"))


```

```{r models, include=FALSE}

benchmarks <- system_performance %>% 
  filter(model == "fri" | model == "lag(1)") %>% 
  select(model, system, rmse) %>% 
  pivot_wider(names_from = model, values_from = "rmse")

age_benchmarks <- age_performance %>% 
  filter(model == "fri" | model == "lag(1)") %>% 
  select(model, age_group, rmse) %>% 
  pivot_wider(names_from = model, values_from = "rmse")


sys_performers <- system_performance %>% 
  filter(!model %in% c("fri"), !str_detect(model,"ensemble"), !str_detect(model,"lag(1)")) %>% 
  group_by(system) %>% 
  filter(srmse == min(srmse))

ens_sys_perfomers <- system_performance %>% 
  filter(str_detect(model,"ensemble")) %>% 
  group_by(system) %>% 
  filter(srmse == min(srmse))


comp_ens_sys_improvement <- ens_sys_perfomers %>% 
  select(model, system, rmse) %>% 
  left_join(benchmarks, by = "system") %>% 
  ungroup() %>% 
  summarise(imp_on_fri = -mean(rmse / fri - 1),
            min_imp_on_fri = min(-(rmse / fri - 1)),
            max_imp_on_fri = max(-(rmse / fri - 1)),
            imp_on_lag =  -mean(rmse / `lag(1)` - 1),
            n_improv = sum(rmse < fri),
            improv_amnt = -mean((rmse / fri - 1)[rmse < fri]),
            worse_amnt = -mean((rmse / fri - 1)[rmse > fri]))

comp_sys_improvement <- sys_performers %>% 
  select(model, system, rmse) %>% 
  left_join(benchmarks, by = "system") %>% 
  ungroup() %>% 
  summarise(imp_on_fri = -mean(rmse / fri - 1),
            min_imp_on_fri = min(-(rmse / fri - 1)),
            max_imp_on_fri = max(-(rmse / fri - 1)),
            imp_on_lag =  -mean(rmse / `lag(1)` - 1))

tmp <- sys_performers %>% 
  select(model, system, rmse) %>% 
  left_join(benchmarks, by = "system") %>% 
  ungroup() %>% 
  mutate(imp_on_fri = -(rmse / fri - 1),
            min_imp_on_fri = (-(rmse / fri - 1)),
            max_imp_on_fri = (-(rmse / fri - 1)),
            imp_on_lag =  -(rmse / `lag(1)` - 1))

age_performers <- age_performance %>% 
  filter(!model %in% c("fri", "boost_tree_ensemble")) %>% 
  group_by(age_group) %>% 
  filter(srmse == min(srmse))

tmp2 <- age_performers %>% 
  select(model, age_group, rmse) %>% 
  left_join(age_benchmarks, by = "age_group") %>% 
  ungroup() %>% 
  mutate(imp_on_fri = -(rmse / fri - 1),
            min_imp_on_fri = (-(rmse / fri - 1)),
            max_imp_on_fri = (-(rmse / fri - 1)),
            imp_on_lag =  -(rmse / `lag(1)` - 1))

a = tmp %>%
  ggplot(aes(reorder(system, -imp_on_fri), imp_on_fri, fill = model)) +
  geom_hline(aes(yintercept = 0)) +
  geom_col(color = "black") +
  scale_y_percent(name = "% Improvement on FRI") +
  scale_x_discrete(name = '')+
  fishualize::scale_fill_fish_d(name = 'Best Model', option = "Trimma_lantana") + 
  theme(axis.text.x = element_text(size = 10),
        axis.text.y = element_text(size = 14)) 

ggsave( "rep_fig_1.pdf",a, device = cairo_pdf, width = 6, height = 4)

b = tmp2 %>%
  mutate(age_group = str_replace_all(age_group,"_",".")) %>% 
  ggplot(aes(reorder(age_group, -imp_on_fri), imp_on_fri, fill = model)) +
  geom_hline(aes(yintercept = 0)) +
  geom_col(color = "black") +
  scale_y_percent(name = "% Improvement on FRI") +
  scale_x_discrete(name = '')+
  fishualize::scale_fill_fish_d(name = 'Best Model', option = "Trimma_lantana") + 
  theme(axis.text.x = element_text(size = 10),
        axis.text.y = element_text(size = 14)) 

ggsave( "rep_fig_2.pdf",b, device = cairo_pdf, width = 6, height = 4)



d <- total_forecast %>%
  filter(model %in% c("boost_tree", "fri")) %>%
  group_by(year) %>%
  mutate(observed = mean(observed)) %>%
  pivot_wider(names_from = "model", values_from = "forecast") %>%
  ggplot() +
  geom_hline(aes(yintercept = 0)) +
  geom_area(aes(year, observed),fill = "lightgrey") +
  geom_point(aes(year, boost_tree, fill = "boost_tree"), shape = 21, size = 4,alpha = 0.9) +
  geom_point(aes(year, fri, fill = "UW-FRI"), shape = 21, size = 4,alpha =0.9) + 
  fishualize::scale_fill_fish_d(name = '', option = "Trimma_lantana") + 
  scale_y_continuous(name = "Total Returns") + 
  scale_x_continuous(name = 'Year') + 
    theme(axis.text.x = element_text(size = 10),
        axis.text.y = element_text(size = 14)) 


ggsave( "rep_fig_3.pdf",d, device = cairo_pdf, width = 6, height = 4)

fri_system_mape <- system_forecast %>% 
  filter(model == "fri") %>%
  group_by(model) %>% 
  summarise(mape = yardstick::mape_vec(observed, forecast))

fri_age_mape <- age_forecast %>% 
  filter(model == "fri") %>%
  group_by(model, age_group) %>% 
  summarise(mape = yardstick::mape_vec(observed, forecast))

# system_forecast %>% 
#   ggplot(aes(observed, forecast)) + 
#   geom_point() + 
#   geom_abline(aes(slope = 1, intercept = 0)) +
#   facet_wrap(~model)


mean_srmse_delta <- system_performance %>% 
    filter(!model %in% c("fri", "boost_tree_ensemble")) %>% 
  group_by(system) %>% 
  mutate(rmse_rank = rank(rmse)) %>% 
  filter(rmse_rank <= 2) %>% 
  arrange(system) %>% 
  group_by(system) %>% 
  summarise(delta  = srmse[rmse_rank == 2] - srmse[rmse_rank == 1]) %>% 
  ungroup() %>% 
  summarise(mmd = mean(delta))

comp_sys_improvement <- sys_performers %>% 
  select(model, system, rmse) %>% 
  left_join(benchmarks, by = "system") %>% 
  ungroup() %>% 
  summarise(imp_on_fri = -mean(rmse / fri - 1),
            min_imp_on_fri = min(-(rmse / fri - 1)),
            max_imp_on_fri = max(-(rmse / fri - 1)),
            imp_on_lag =  -mean(rmse / `lag(1)` - 1))


sys_improv <- system_performance %>% 
  select(model, system, srmse) %>% 
  pivot_wider(names_from = model, values_from = srmse) %>% 
  pivot_longer(-c(system, fri), names_to = "model", values_to = "srmse") %>% 
  group_by(system) %>% 
  filter(srmse == min(srmse)) %>% 
  ungroup() %>% 
  summarise(srmse_improv = mean((srmse - fri) / fri))

age_improv <- age_performance %>% 
  select(model, age_group, srmse) %>% 
  pivot_wider(names_from = model, values_from = srmse) %>% 
  pivot_longer(-c(age_group, fri), names_to = "model", values_to = "srmse") %>% 
  group_by(age_group) %>% 
  filter(srmse == min(srmse)) %>% 
  ungroup() %>% 
  summarise(srmse_improv = mean((srmse - fri) / fri))


pearson_residuals <- age_system_forecast %>% 
  group_by(model) %>% 
  mutate(pearson_residual = (forecast - observed) / sd(forecast))
  
pearson_residuals %>% 
  ggplot(aes(model, pearson_residual)) + 
  geom_boxplot()

pearson_residuals %>% 
  ggplot(aes(pearson_residual, fill = model)) + 
  geom_density(alpha = 0.25)


system_r2 <- top_system_forecast %>% group_by(system) %>% yardstick::rsq(observed, forecast)

age_r2 <- top_age_forecast %>% group_by(age_group) %>% yardstick::rsq(observed, forecast)

ens_r2 <- top_ensemble_system_forecast %>% group_by(system) %>% yardstick::rsq(observed, forecast)
  
# forecast_plot <- forecasts %>% 
#   mutate(id = paste(system, age_group, sep = ":")) %>% 
#   ggplot(aes(observed, forecast, color = year)) + 
#   geom_point() + 
#   facet_grid(id~model, scales = "free_x") + 
#   theme_minimal_grid()

```

\newpage

# Title Page {.unnumbered}

## Title   {.unnumbered}

<!-- Improving Forecasts of Sockeye Salmon (*Oncorhynchus nerka*) with Modern Computational Tools.  -->

Improving Forecasts of Sockeye Salmon (*Oncorhynchus nerka*) with Parametric and Non-Parametric Models


## Author List {.unnumbered}

Daniel Ovando^a^\*, Curry Cunningham^b^,Peter Kuriyama^c^,Christopher Boatright^a^, Ray Hilborn ^a,d^

^a^ School of Aquatic and Fishery Sciences, University of Washington Box 355030 Seattle WA 98195

^b^College of Fisheries and Ocean Sciences University of Alaska Fairbanks 17101 Point Lena Loop Road Juneau, AK 99801

^c^NOAA Fisheries Southwest Fisheries Science Center 8901 La Jolla Shores Dr, La Jolla, CA 92037

^d^Center for Sustaining Seafood, University of Washington Box 355030 Seattle WA 98195

## Corresponding Author {.unnumbered}

\*Correspondence should be addressed to Daniel Ovando at [danovan\@uw.edu](mailto:danovan@uw.edu){.email}


\newpage

## Abstract {.unnumbered}

Accurate forecasts of sockeye salmon (*Oncorhynchus nerka*) in Bristol Bay, Alaska, play an important role in management and harvesting decisions for this culturally and ecologically vital species. We used a suite of parametric and non-parametric models to assess the frontiers in forecast accuracy of Bristol Bay sockeye salmon possible given currently-available data. In retrospective performance testing individual models were capable of reducing pre-season forecast error at the river system level by on average `r percent(comp_sys_improvement$imp_on_fri)` relative to a benchmark model. We utilized an ensemble modeling approach to produce pre-season forecasts based on historic performance of individual models. This ensemble model reduced river system level forecast error by `r percent(comp_ens_sys_improvement$improv_amnt)` on average in `r comp_ens_sys_improvement$n_improv` of the `r n_distinct(system_forecast$system)` evaluated river systems, though it increased forecast error by `r percent(-comp_ens_sys_improvement$worse_amnt)` on average in the remaining `r n_distinct(system_forecast$system) - comp_ens_sys_improvement$n_improv` systems. We found potential for modest improvements in forecast accuracy across a variety of scales. However all tested models failed to accurately predict certain periods in the historic salmon return patterns, indicating that further forecast improvements likely depend on novel data rather than more flexible models.

<!-- Our results demonstrate both the utility and limitations of modern computational tools for applied management of natural resources, even when confronted with the short time series and observation error common in ecological systems. -->

<!-- A persistent question is whether pre-season forecast accuracy is primarily limited by the information content of the available data, or the degree of misspecification of the models applied to the data.  -->

## Keywords {.unnumbered}

Alaska, Salmon,Machine learning,Empirical Dynamic Modeling, Dynamic Linear Models, Predictive Modeling, Ecological Forecasting

\newpage

<!-- ## Significance Statement {.unnumbered} -->

<!-- The sockeye salmon (*Oncorhynchus nerka*) populations of Bristol Bay, Alaska are economic and ecological marvels, playing key roles in the ecosystems of Alaska and supporting the largest commercial salmon fishery in the world. Pre-season forecasts of the annual returns of these fish are important to both managers and fishing fleets. Computational tools such as machine learning have transformed predictive fields from advertising to weather forecasting. However, these methods are underutilized for managing ecological systems. Using modern computational tools we were able to substantially increase the accuracy of pre-season forecasts for the Bristol Bay sockeye salmon fishery. Our results demonstrate the utility and limitations of modern computational tools for applied management of natural resources, even when confronted with the short time series and observation error common in ecological systems. -->

# Introduction

Animal populations exhibit complex dynamics driven by interactions with many aspects of their ecosystem. Predicting the outcomes of these dynamics is a critical task of natural resource management; Forecasts of future abundance are often used to set fisheries regulations, vessel operators may make decisions about alternative fisheries based on predicted abundance, and industries and communities use forecasts to inform long-term and short-term investment plans in staffing and production capacity. The past two decades have seen explosive progress in the ability of modern "computer age" [@efron2016] parametric and non-parametric models to improve prediction, revolutionizing fields such as financial modeling, weather forecasting, and medicine. However, these predictive methods are still uncommon in applied ecological forecasting [@peters2014]. We use the ecologically and economically critical sockeye salmon (*Oncorhynchus nerka*) populations of Bristol Bay, Alaska to assess the potential of a range of predictive modeling techniques to improve pre-season forecasts and identify frontiers in forecast accuracy achievable given currently available data.

<!-- However, the methods used to make these ecological predictions are often decades old and generally do not go beyond forward projection from age-structured models. -->



Sockeye salmon are semelparous,  born in freshwater where they spend the first one or more years of their lives. Eventually, these fish migrate to the ocean, where they remain until returning to their natal river systems to spawn and then die. Sockeye salmon exhibit life history variation in the number of years they spend in these freshwater and oceanic phases, representing distinct “age groups”. Following conventions in the salmon literature, we denote age groups here by the format “years spent in freshwater.years spent in the ocean.” For example, a fish in the 1.2 age group spent one winter post-hatching in freshwater before migrating to sea two years after it was spawned, and two winters in the ocean before returning to freshwater to spawn. The Bristol Bay sockeye salmon fishery is primarily made up of salmon from seven different river systems, each of which is managed as a separate stock (Fig.\@ref(fig:returns)).

The commercial salmon fishery in Bristol Bay, Alaska is the single largest sockeye salmon fishery in the world [@steiner2011]. The estimated wholesale value of the Bristol Bay commercial sockeye harvest was \$390 million USD in 2010, providing approximately one-sixth of the total value of all United States seafood exports [@knapp2013]. The value of the Bristol Bay fishery has continued to grow, reaching \$508 million in 2017 [@mcdowellgroup2018]. Salmon returning to Bristol Bay also provide vital food security for subsistence-dependent Alaskan communities, and are critical vectors of marine-derived nutrients that support vibrant freshwater habitats [@naiman2002; @schindler2003]. Sustainable management of the Bristol Bay salmon fishery depends in part on the accuracy of pre-season forecasts for salmon abundance, which inform development and implementation of inseason harvest strategies and successful operation of subsistence and commercial fisheries. While in-season forecasts updated as evidence for the strength and timing of a run accrue are vital for management of many salmon stocks, pre-season forecasts are also important for planning by the processing industry, as a basis for identifying the appropriate level of supplies, equipment, and personnel necessary to process the annual harvest. As such, the accuracy of salmon forecasts have a direct influence on the profitability and efficiency of the salmon industry as a whole, particularly for stocks with a shorter harvest window relative to the time needed to plan and adjust fishing and management actions.



```{r returns, fig.cap = "Annual total abundance of returning sockeye salmon (*Oncorhynchus nerka*) to Bristol Bay, Alaska (A), by river system (B), and by age group (C). Numbers in millions of salmon. Age group is formatted by ‘years spent in freshwater’.‘years spent in ocean’. Map adapted from @cunningham2019.", fig.width=7, fig.asp=.8}
return_plot
```



```{r}
fri_mape <- system_forecast %>% 
  filter(model == "fri") %>% 
  yardstick::mape(observed, forecast)

```


The Fisheries Research Institute (FRI) at the University of Washington and the Alaska Department of Fish and Game (ADFG) have been providing pre-season forecasts for the annual abundance of sockeye salmon returning to the major river systems and fishing districts of Bristol Bay since at least 1967 (Cunningham pers. comm.). While the exact statistical methods used for FRI and ADFG forecasts have evolved over time, throughout their history they have primarily been based on the relationship between the abundance of successive age classes of salmon returning in different years. While these traditional forecast methods have been useful in guiding decisions by fishers, processors, and managers alike, improvements in the accuracy and precision of pre-season forecasts would represent a valuable advance.

<!-- Between the years 2000 and 2020, the mean absolute percent error (MAPE) of total sockeye returns across the seven major river systems of the published FRI forecast has been `r scales::percent(fri_mape$.estimate / 100)`. How might we improve on these results?  -->

There exists some "frontier" of maximum predictive ability contained in the available data used to make forecasts. However, a model might perform far below the predictive frontier attainable given available data if it is severely misspecified, i.e. if the assumptions of the model do not properly reflect reality. Identifying the best predictive model in an ecological setting has conventionally been achieved by manual construction and comparison of competing models via some information criteria. However, this can be a cumbersome process, particularly as the number of covariates and subsequently the number of potential interactions and non-linearities increases. 

Alternatively, non-parametric methods such as machine learning excel at identifying and exploiting potentially complex correlations between variables in a system. Parametric statistical methods often restrict themselves to simplified (e.g. linear) and often non-dynamic representations of natural systems, both for analytical tractability and to facilitate heuristic understanding of underlying processes. Typically, these parametric statistical methods are concerned with explicitly estimating and interpreting model parameters rather than solely forecasting responses, such as population size [@beyan2020; @malde2020]. We would expect non-parametric methods to show substantial improvement in predictive power when the "true" underlying system linking observed variables and outcomes differs dramatically from the simplified representations of the system approximated by more parametric statistical approaches. In the case of salmon, we know that inter-annual variation in run sizes is affected by a wide range of ecosystem variables, including spawning success, river conditions, oceanic predator and prey abundance, and competition with other salmonids [@connors2020]. By reducing the potential for predictive model misspecification, non-parametric models that essentially seek to “learn” the best model structure for the sole purpose of maximizing out-of-sample predictive performance can provide a test of the predictive information content of the available data themselves. Utilizing multiple parametric and non-parametric modeling approaches allows us to develop a clearer picture of this frontier in predictive ability achievable given a certain set of data. 


Salmon forecasting has traditionally relied on cohort or “sibling” regression methods, in which the return abundance of an older age class is predicted by the abundance of younger age classes, returning in prior years but originating from the same river system and brood year. For example, the return abundance of four-year old fish are predicted by the returns of three-year old fish observed in the previous season. There are good reasons for this practice: trends in sibling abundance integrate across many environmental factors affecting salmon survival and returns. If a particular cohort suffers from poor environmental conditions, increased competition with conspecifics,  or a greater abundance of predators, the demographic impacts of these changes will be reflected in the return abundance of younger age classes from the same cohort (i.e. originating from the same brood year) that experienced similar environmental conditions or resource availability, and by extension survival, as juveniles. 

However, the standard sibling regression method does have shortcomings, most notably the underlying assumption of consistency in the relationship between the abundance of different age classes and stability in the maturation schedule (i.e. the probability of salmon maturing and returning to freshwater to spawn after a given number of years in the ocean). For example, if environmental conditions cause members of a cohort of salmon to spend more time at sea than in previous years, a sibling regression might under-predict the number of future returns. In addition, sibling regression requires accurate observations of the return abundance for younger sibling age classes, limiting the performance of these models in predicting returns of younger salmon for which few or no siblings (i.e. returning younger age classes) have yet been observed.

We hypothesize that directly incorporating data on candidate potentially time-varying factors influencing and correlated with salmon return size, rather than relying on sibling returns alone, may help improve forecast performance given the complex dynamics of salmon populations. However, these variables are likely to have complex, non-linear, and non-stationary effects on salmon populations, potentially obscuring their value from conventional parametric statistical approaches with user-defined parameters, structures, and error distributions. In order to explore this possibility, we used a suite of four methods together with a panel of data on salmon populations and environmental conditions in Bristol Bay, Alaska to explore what if any improvements in forecast skill could be achieved. These models included two machine learning methods (a random forest (rand\_forest), [@breiman2001; @wright2017] and a boosted regression tree, boost\_tree, [@chen2020]), empirical dynamic models (edm) [@sugihara1990; @munch2020; @ye2020], and dynamic linear models (dlm)[ @petris2009; @pole1994]. We compared each model to the performance of a lag(1) model in which the predicted returns for a given age group and river system in a year are equal to the observed returns for that that age group in that river system in the prior year.  We also evaluated the performance of a model ensemble which weights predictions from individual ensemble members (alternative predictive model types) based on recent performance, and compare this with observed performance from the Fisheries Research Institute (FRI) forecast, a benchmark forecast that utilizes a qualitative ensemble approach based on evaluation of recent performance for alternative models within the ensemble. 

Other studies have incorporated various time-varying parametric and non-parametric models, including versions of the models used here, for salmon forecasting [@holt2004;@velez-espino2019;@dfo2018;@yi2019]. Our study builds on this literature not by seeking to establish whether one type of model performs inherently better than others, but by examining the ability of a suite of these models to collectively improve salmon forecasting by leveraging correlations both within and among river systems and age classes in Bristol Bay.  In doing so, we demonstrate how collections of parametric and non-parametric models can be used to identify frontiers in forecasting ability available in a given dataset. 


# Materials and methods 

All code and data needed to fully replicate our results are publicly available at <https://github.com/DanOvando/salmon-forecast-paper/>. We describe critical details of each our main methods here. 

The general structure of our methods are as follows: 

1. Individual models for each river system and age group were fit to historical data 

2. Retrospective performance of individual model was assessed using 1-step ahead predictions (e.g. model fit to data through 1999 and used to predict return abundance in 2000) over the period 2000-2020 

3. Comparison of performance from individual models types against a benchmark  “lag(1)” prediction model in which the forecast for next year is simply the observed returns in the previous year

3. Individual models were aggregated into a statistical ensemble model based on their historic performance against the lag(1) benchmark

4. The statistical ensemble model was then compared to a more qualitatively constructed ensemble model, in which researchers manually select individual models from an evolving suite of methods based on recent (20-year) performance. This is the method historically used to generate Bristol Bay salmon forecasts, although the individual prediction models within the selection suite have changed over time. As such this method provides a *status quo* benchmark to which individual models and statistical ensembles may be compared. 


## Data

### Salmon returns


The primary data behind this analysis are historic numbers of sockeye salmon by age group returning to each of the seven Bristol Bay river systems considered here (Fig.\@ref(fig:returns)). We included data from 1963 through 2020, omitting pre 1963 data as that year marks a major change in the data collection methods. We generated forecasts for the four most prominent age groups in the data, the youngest of which being the 1.2 age group. However, we include data from younger age groups as candidate covariates. For example, returns of 1.1 fish in the year 2000 are used to generate forecasts of 1.2 fish in the year 2021, even though we never forecast 1.1 fish explicitly. In this manner all forecasts generated by our model can be based at least in part on previous observed siblings.  

The dynamics of the Bristol Bay salmon runs changed dramatically from their historic patterns starting in the 1980s (Fig.\@ref(fig:returns)). We chose to include data from before and after this change, rather than fitting to data from the more recent regime only, as exploratory analyses found better forecast accuracy resulting from inclusion of the full dataset. Since each of the models used here have the capacity for time varying parameters,  including data from before and after 1980 allows in theory for the model to leverage shared patterns across the two regimes, while also theoretically learning about changes in patterns over this time period. All models were fit on the raw unit-scale (i.e. not log-transformed) returns, as we found better performance with this route than through log-transformation. 



### Additional Covariates

Along with sockeye salmon returns, we also included several environmental and salmonid datasets as potential covariates (Table.\@ref(tab:dat)). Environmental data included the strength of the Pacific Decadal Oscillation (PDO), sea surface temperature, sea level pressure, and wind stress. Each of these variables were included as the mean or median of the values of that index over the Bristol Bay area between May and August of the year in which the cohort being forecasted would have entered the ocean. Our assumption here is that this early oceanic period represents a critical stage in the survival of sockeye. We tested treating gridded values of environmental covariates over space and time as predictors (rather than aggregating to the Bristol Bay wide value), in theory allowing the models to learn which locations and times were the most useful predictors, but found this approach to perform poorly, likely due to the sample size available. Environmental datasets were queried from the NOAA ERDDAP portal using the rerddap package in R [@chamberlain2019]. We also included as candidate covariates natural origin returns of pink (*Oncorhynchus gorbuscha*) and chum (*Oncorhynchus keta*) salmon from a range of North Pacific stocks, pulled from @ruggerone2018. While all of the models used in this paper are capable of including all of the covariates included in Table.\@ref(tab:dat) in some manner, only the machine learning models made use of these data. 

```{r dat, results="asis"}

tmp <-  system_importance_table %>% ungroup() %>% select(name, Description, Source) %>% rename(Name = name) %>% unique() %>% filter(
    !Name %in% c("Return Year", "Spawner Numbers"),
    !str_detect(Name, "Past")
  ) %>% arrange(Name) %>% 
  filter(!str_detect(Name,":")) %>% 
  as.data.frame()

salmonind_stuff <- tribble(~Name, ~Description, ~Source,
                           "Pink and chum abundance","Natural origin returns of pink and chum salmon",
                           "Ruggerone and Irvine (2018)")

tmp <- tmp %>% 
  bind_rows(salmonind_stuff)

knitr::kable(tmp,
             digits = 2,
             caption = "Environmental and salmonid datasets available to machine learning models.",
             booktabs = TRUE,
             format = "latex") %>%
  kableExtra::kable_styling(full_width = TRUE)


```



## Machine Learning Models 

We evaluated two different machine learning models: a random forest (rand\_forest, implemented through the `ranger` package [@wright2017] in R [@rcoreteam2020]), and boosted regression trees, boost\_tree, through the `xgboost` package [@chen2020]. A recurrent neural network implemented through `tensorflow` [@allaire2020] through the `keras` interface [@allaire2020a] was also tested but was found to perform poorly relative to the other methods and to be extremely computationally intensive, and as such was not included in the main analysis. Random forests are ensembles of regression trees, which make predictions by selecting nested splits of variables and mapping the mean level of the dependent variable at the terminal nodes of each tree. Boosted regression trees are similar to random forests, but have mechanisms in place that actively update the model to address data points that the model is struggling to fit [@elith2008]. For all machine learning methods, within a model fit the model selects splits/transformations/coefficients to minimize the root mean squared error (RMSE) of predictions for data withheld from the fitting process by the algorithm.

 Both the random forest and boosted regression tree models had access to the same data. These data included transformations of both the environmental and salmonid data in Table(\@ref(tab:dat)) and the historic return data. For the environmental and salmonid data, we calculated the cumulative mean value for each variable experienced by the cohort in question during its oceanic phase.  For the historic return data, the predictors for a given cohort are all the observations of that cohort in previous years across all river systems. For example, if the model is currently predicting the 1.3 age group, the return covaraites would be the returns of 1.2 fish in all river systems in the prior year, the returns of 1.1 fish in all river systems the year before that, and so on. We also calculated the number of spawners that produced each cohort and used that as a predictor. Some data were missing for the earliest years in the data (e.g. spawning numbers for cohorts born before 1963), and these were imputed from the most recent years with available data for that river system. 
 
 We fit versions of each model separately for each age group in each major river system. We tested versions of the models that fit the age groups and river systems simultaneously, but did not use this approach as it performed worse than the individual approach, likely due to our limited sample size. When fitting models at the level of age groups by river system, data were first split into rolling training and testing sets. For example, if the goal is to forecast returns in the year 2001, all data prior to 2001 were used as the training data, and all data post 2000 were set aside as the "testing" data. We then split each of the training sets for performance testing (e.g. all data before the year 2000 if the year 2001 is to be forecasted) into a series of analysis and assessment splits for tuning purposes. Given the timeseries nature of the data, we generated these analysis and assessment splits in a rolling manner. For example, for the first split, we used the first 70% of the training data as the analysis data to fit a model, and the remaining 30% of the training data as an assessment split to evaluate the performance of that model. For the next split we used the first 75% of the training data for the analysis split and 25% for the assessment split, and so on. These analysis and assessment splits were used to tune nuisance parameters common to all machine learning models, for example the minimum node size of fitted trees (see Table.S3 for a complete list of tuning parameters). We fit each of our assessment splits across a grid of potential parameter values, and selected the set of tuning parameters that minimized the RMSE of the predictions on the assessment splits (see computational environment available at <https://github.com/DanOvando/salmon-forecast-paper/> for detailed steps in this process).

Once the optimal set of tuning parameters for each training set were selected, we then fit the final model using all the training data with those tuned parameters, and used that model to predict the returns in the testing set. This process was repeated for forecasting year. All relevant data transformations were prepared only on training / analysis splits (e.g. means and standard deviations for centering and scaling) and then applied to testing / assessment splits. 


## Dynamic Linear Models 

To date methods for forecasting sockeye salmon abundance in Bristol Bay and throughout Alaska have generally relied on the relationship between the abundance of different age classes from the same cohort, or originating from the same brood year, but returning to breed in subsequent years at different ocean ages. Foundational to the predictability of sibling relationships is the assumption that the ratio of returns by age class remains stable across time. In a context of a linear model, for example, we can model returns as $\hat{R^{1.3}_t} = \alpha + \beta{R^{1.2}_{t-1}}$ where $\hat{R^{1.3}_t}$ is the predicted return abundance of the older (1.3) age class and $R^{1.2}_{t-1}$ is the observed abundance of the same cohort returning in the prior year after one fewer years in the ocean (i.e. age group 1.2). Under a classic sibling regression the assumption is that the estimated parameters $\alpha$ and $\beta$ remain constant across time. However, there are multiple conditions under which both the average return abundance of a particular age class or the ratio of abundances among age classes might change over time. For example, if the average maturation schedule (i.e. the probability that an individual will mature after 2 vs. 3 years in the ocean) changes in response to natural or anthropogenic selection, the assumption of a stationary parameter is violated. Alternatively, if average marine mortality experienced by salmon changes as a result of large-scale climate, ecosystem, or trophic shifts, this should be reflected by changes in both parameters of the regression model.

To better represent the dynamic nature of sibling or cohort relationships over time and improve predictive performance, we implement dynamic linear models (DLMs). DLMs are a class of regression models where the values of regression coefficients are permitted to evolve over time, rather than remain static [@petris2009; @pole1994]. DLMs were fit to available data using a single predictor age class (one fewer year in the ocean, returning the prior year), and allowing for evolution of both the slope and intercept parameters over time, as:

$$\hat{R^{1.3}_t} = \alpha_t + \beta_t{R^{1.2}_{t-1}} + \epsilon_t$$

Both regression parameters are described by a random walk (i.e. $\alpha_t \sim Normal(\alpha_{t-1},\sigma^2_{\alpha})$ and $\beta_t \sim Normal(\beta_{t-1},\sigma^2_{\beta})$), and errors were assumed normally-distributed ($\epsilon_t \sim Normal(0,\sigma^2_{\epsilon})$). DLMs were implemented using the Multivariate Autoregressive State-Space Modelling (MARSS) package (version 3.10.12) in R [@holmes2012; @holmes2020]. The full timeseries (brood year 1963 forward) of age and river system specific abundances reconstructed by @cunningham2019  were for model fitting. For example, to predict the abundance of the 1.2 age class returning to the Wood River system in 2010, the DLM model was fit to available data 1963-2009, with the 1.1 age class in prior years assumed *a priori* to be the most informative sibling abundance predictor. Given the random walk structure of these dynamic linear models, it is implicitly assumed that both the average abundance and the empirical relationship between age classes for the terminal forecast year are most similar to values for those parameters observed in the recent past. this is in contrast to the machine learning and empirical dynamic modeling approaches that are more flexible in this regard. 

## Empirical Dynamic Modeling 

Empirical dynamic modeling (EDM) is a nonparametric approach to characterize ecological dynamics and generate forecasts. The approach is predicated on Takens' theorem, which states that a single time series and a number of lags (dimension; E) are representative of overall system dynamics [@sugihara1990; @takens1981]. Different types of EDM have identified causal relationships in ecological systems [@sugihara2012] and improved forecast skill in Fraser River sockeye salmon [@ye2015]. See @munch2020 and @chang2017 for more general overviews of EDM. We used the software package rEDM [@park2020] for our analysis.


We focused on multiview embedding form of EDM [@ye2016] to predict Bristol Bay sockeye returns. We predicted out-of-sample river and age-class-specific returns for 2000-2019. The idea behind multiview embedding is that there are potentially many valid reconstructions of system dynamics, and evaluating possible different combinations may improve performance. The top Multiview embedding was identified with river-specific data with a maximum number of dimensions (E) of two. Multiview embedding selects models based on the within-sample fits. So to predict say Kvichak 2.2 returns in the year 2000, we subset data through 1999 for Kvicahk 1.2, 1.3, 2.3, and 2.2, then selected the multiview embedding that had the highest within-sample predictive skill. We evaluated embeddings with maximum dimensions up to E=4, although this increase did not consistently result in improved within-sample predictive skill, perhaps due to noise in the data. 

We present results from multiview embedding but we also evaluated additional EDM approaches. These included multivariate simplex, multivariate sequentially locally weighted global linear maps (s-map), and composite libraries for prediction to the salmon return data. These methods require identifying the dimensionality (E) of a time series and constructing an attractor (a time series and its E-lagged coordinates). Leave-one-out prediction identifies the best E of a time series. We used E values ranging from 1 to 10, found the E-nearest neighbors (based on Euclidean distance) from the observation of interest, and calculated a predicted value by averaging the E-nearest neighbors. The best E had the highest correlation between observed and predicted values. S-maps is an extension of simplex that has the addition of a weighting parameter (theta) which modifies the strength of nearest neighbor weighting (theta=0 weights nearest neighbors equally; theta>0 stronger weighting of nearest neighbors) [@sugihara1994]. Across these tests, the multiview method with E=2 was the best performed, and as such is what we present here. 

## Performance Metrics 

We do not conduct formal statistical tests of model fit or performance. Parameters of conventional statistical models might be assessed in terms of statistical significance, and models compared via some form of information criterion. However, neither the machine learning or the empirical dynamic modeling methods have formal estimates of uncertainty or likelihoods, and as such do not produce measures of statistical significance around individual forecasts, and cannot be compared using information criteria such as AIC [@akaike1974] scores. Accordingly, we judge model performance by the point estimates of SRMSE produced by each model across the retrospective horizon 1990-2020 SRMSE measures the performance of each model relative to a lag(1) model, a conventional benchmark model for timeseries modeling [@hyndman2006;@ward2014]

RMSE is calculated as

$$RMSE_m = \sqrt{\frac{1}{I}\sum\limits_{i = 1}^I(y_{i} - f_{i,m})^2}$$

where *i* represents an observation of numbers of returning salmon *y* and the forecast for those numbers *f* by a given model *m*. In the manner of mean absolute scaled error (MASE, [@hyndman2006]), we scale each models RMSE for a given resolution by the RMSE of a lag(1) model for the same resolution (for example at the river system level).

$$RMSE_{l1} = \sqrt{\frac{1}{I}\sum\limits_{i = 1}^I(y_{i} - y_{i,l1})^2}$$

and SRMSE for model *m* is then

$$SRMSE_m = \frac{RMSE_m}{RMSE_{lag(1)}}$$

MASE is commonly used to judge the accuracy of predictions derived from time series models, since it compares the error of a given model to the error expected by a simple model in which the predictions in a given time step are equal to the observed values in the last time step (a lag(1) model). We use SRMSE instead of MASE to reflect the use of the forecast. MASE considers an error of ten to be twice as bad as an error of five. In the context of salmon forecasting, our primary objective is to avoid massively over or under estimating the pre-season forecast. SRMSE penalizes large errors more than small errors, helping select models that avoid the kinds of large errors that are most problematic for the task of managing salmon populations. Bias is also of importance in judging a forecast, and we include summaries of bias performance in the Supporting Information (SI). 

A SRMSE of one means that a model has predictive performance equal to that of the lag(1) model. A SRMSE greater than one indicates that a given model performs worse than the lag(1) benchmark, a SRMSE less than one that a model performs better than the lag(1) benchmark [@hyndman2006]. We also calculated the predictive R^2^ for each of our relevant results. 

<!-- Forecasts for a given year are produced by a model trained on data from all preceding years. This type of sliding-window cross-validation permitted the most realistic evaluation of retrospective model performance. -->

## Testing Regime 

All models were compared based upon one-step-ahead forecast skill, defined by SRMSE. Each of the evaluated models generate forecasts at the resolution of age group and river system in a given year. Forecasts for a given year are produced by a model trained on all years after 1963 and prior to the year for which a forecast is desired. This is performed in a rolling fashion, such that for example forecasts for the year 2018 are produced by a model trained on data from 1963 to 2017, the 2019 forecast by a model trained on data from 1963 to 2018, and so on. One-step-ahead performance skill was preferred over simple leave-one-out cross validation because it better aligns with the context of pre-season forecasting (i.e. data in hand through the current year are used to predict the next), and should be expected to more appropriately reflect true forecast uncertainty in the presence of periodic regime shifts in salmon production and the potential for unmodeled autocorrelation. Each method has its own ways of tuning and validating the model, but all such steps are performed using only the training data: all data for the forecast year are held out until the final prediction.

Predictive performance of candidate models was calculated by generating 1-year ahead forecasts for each target river system by age class combination, as a rolling window from the year 2000 to 2020 This method for quantifying forecast performance is most applicable to the context of this ecological forecasting problem as each candidate model is trained on data up to, but not including, the prediction year. Even though each model generates predictions at the resolution of age group and river system, we generally compare model performance at coarser resolutions (for example river system across all age groups). In those cases, we first aggregated the total returns at the resolution in question (e.g. summing all observed and forecast returns across all age groups for a given river system), and then calculated the SRMSE based on those aggregated data. This allowed the best model to differ based on the scale of the predictions. For example, the model that performed best when measured at the level of age group and river system by not be the model the performed best in terms of total system returns. 



## Ensemble Models 

The chosen testing regime allowed us to compare the retrospective predictive power, defined by SRMSE, of individual models at a variety of spatial resolutions. However, scientists must make a decision each year as to which models to use for particular forecasts, and there is no guarantee that past model performance will predict future model performance. A substantial body of literature suggests that creating "ensemble" models that weigh individual models to create a single composite prediction can outperform any one individual model  [@anderson2017a; @araujo2007; @Dietterich2000]. To assess the ability of this idea to assist in annual model selection and weighting we compared two different ensemble models: a purely statistical ensemble constructed by a random forest, and a mixed-methods ensemble model published as the Fisheries Research Institute (FRI) forecasts. 

<!-- Both of these ensembles generate model-of-model-predictions which we can compare to the predictions made by individual models.  -->

The random forest ensemble model was updated each year by evaluating the performance of different models in the past and creating a prediction for the current time step based on the performance of component models (the ensemble members) in the prior time steps. For the random forest ensemble, we predicted the total returns by river system as a function of the predictions by river system and age group from each individual candidate model type. Along with the four models estimated in our results, the random forest also had access to the baseline lag(1) model, in case the data suggest that in fact this baseline model is preferable to or in some way complements the four statistical models included. A conventional ensemble might be constructed by taking an AIC weighted mean of forecasts of each of the candidate models for a particular river system. By constructing an explicit "model-of-models" ensemble through a random forest, we allow the choice of model weighting to vary depending on the performance of different models in different river systems and time periods [@anderson2017a].

<!-- The FRI forecast is a mixed-methods ensemble model manually constructed by FRI scientists, which has used various models throughout the years to arrive at pre-season forecasts for each river system based on the recent performance of different candidate tools. The FRI forecast for a specific stock by age class combination was traditionally constructed by AIC-weighting across candidate linear sibling models. Candidate linear models predict returns of the target age class using returns of one or two younger age classes seen in prior years as predictors. These candidate predictive models were fit on two alternative time series, 1963 onward and 1980 onward, to account for broad-scale shifts in average Bristol Bay salmon population productivity following the shift in the Pacific Decadal Oscillation (PDO) in the late 1970's. Natural-scale and log-transformed transformations were both fit for all models. Since 2013 the FRI forecast ensemble has been constructed by comparing the performance of the linear and log-linear AIC-weighted sibling models, random forest models, dynamic linear sibling models, boosted regression trees, and simple autoregressive integrated moving average (ARIMA) time series models, and selecting the model with the lowest residual error in predictions for the target stock-age group across the most recent 20-year time horizon. -->

The FRI forecast is a mixed-methods ensemble model manually constructed by FRI scientists, which has used various models throughout the years to arrive at pre-season forecasts for each river system based on the recent performance of the component models. The FRI forecast for a specific river by age  class combination was traditionally constructed by AIC-weighting across candidate linear sibling regression models. Candidate linear models predict returns of the target age class using returns of one or two younger age classes seen in prior years as predictors, but unlike the DLM model explored here, assume regression coefficients are time-invariant. These candidate predictive models were fit on two alternative time series, 1963 onward and 1980 onward, to account for broad-scale shifts in average Bristol Bay salmon population productivity following the shift in the Pacific Decadal Oscillation (PDO) in the late 1970’s. Natural-scale and log-transformed transformations were both fit for all models. Since 2013 the FRI forecast ensemble has been constructed by comparing the performance of the linear and log-linear AIC-weighted sibling models, random forest models, dynamic linear sibling models, boosted regression trees, and simple autoregressive integrated moving average (ARIMA) time series models, and selecting the model with the lowest residual error in predictions for the target stock-age group across the most recent 20-year time horizon. All models in the historical FRI ensemble used only data from within a single river system, but across multiple age classes, to generate predictions (i.e. age-specific timeseries of Nushagak River returns were never used to forecast Wood River returns despite their spatial proximity).

The FRI ensemble forecast values were pulled from the historic pre-season forecasts as published. For the random forest ensemble, we follow a similar routine to that employed for the individual (i.e. river- and age-specific) boosted regression tree model. We compiled the pre-season forecasts by river system and age group for each of the candidate models going back to 1991. The ensemble sought to predict the observed total returns by river system using the returns by river system and age class produced by each of the candidate models (BRT, RF, DLM, EDM). For the years 2000 to 2019, we performed a series of rolling model fits, where individual forecasts and observed returns before the testing year was held out for training (and analysis and assessment splitting and model tuning), and then used fit the ensemble model, which was then evaluated on the testing year. The held-out one-year-ahead predictions of the ensemble model in each time step were then compiled to create the historic series of ensemble forecasts at the river system level.


# Results 



<!-- XX Need a brief table of each of the methods describing them and their acronyms in the intro or start of the results XX -->

## Individual Model Forecasts 


### River System Forecasts 

Management of Bristol Bay sockeye salmon operates at the river system level, with inseason fishery managers regulating allowable fishing effort on a daily basis to meet annual escapement goals for each river [@cunningham2019;@fried1988]. For each river system, we selected the individual model with the lowest SRMSE over the years 2000 to 2019 as the model of choice for that river system. On average the best-performing method reduced the SRMSE in pre-season run forecasts at the river system level by `r percent(comp_sys_improvement$imp_on_fri)`, with a minimum improvement of `r percent(comp_sys_improvement$min_imp_on_fri)` and a maximum of `r percent(comp_sys_improvement$max_imp_on_fri)`, relative to the performance of the historic published pre-season FRI forecasts.

River systems varied in both the lowest SRMSE achieved and in the model that produced the best performance. At least one model was able to out-perform or equal a simple lag(1) benchmark model in each of the river systems except for the Nushagak, with the `r  sys_performers$model[sys_performers$srmse == min(sys_performers$srmse)]` model achieving a SRMSE of `r prettyNum(min(sys_performers$srmse), digits = 2)` at the top end in the `r sys_performers$system[sys_performers$srmse == min(sys_performers$srmse)]` river system, and the `r sys_performers$model[sys_performers$srmse == max(sys_performers$srmse)]` model providing only marginal improvement over a lag(1) model with a SRMSE of `r prettyNum(max(sys_performers$srmse), digits = 2)` in the `r sys_performers$system[sys_performers$srmse == max(sys_performers$srmse)]` river system. The dlm, boost\_tree, and rand\_forest models were selected as the best performing candidate in at least one river system (Fig.\@ref(fig:sys-forecast)). R^2^ values at the system level ranged from a low of `r round(min(system_r2$.estimate),2)` in the `r system_r2$system[which.min(system_r2$.estimate)]` to a high of `r round(max(system_r2$.estimate),2)` in the `r system_r2$system[which.max(system_r2$.estimate)]`.

```{r sys-forecast, fig.cap="Observed (grey ribbons) and predicted (points) numbers of returning sockeye salmon to primary sockeye-producing river systems in Bristol Bay, Alaska. The color of the points corresponds to the best performing model in terms of scaled root mean squared error (SRMSE), point transparency reflects the SRMSE of the best performing model, noted in the top left corner of each panel along with the R^2^ value."}
system_forecast_figure

```

### Age Group Forecasts 

While total river system returns are the primary metric of interest to the Bristol Bay sockeye fishery, the age composition of the returns are also important given their influence of the average size and therefore price for each salmon harvested, and options for processed product forms.  As such we also examined the ability of our tested models to generate predictions at the age group level. In retrospect different models performed best for each of the four age groups considered, and at least one model was able to improve substantially on a lag(1) model in all age groups (Fig.\@ref(fig:age-forecast), SRMSE <1 in all cases, with at minimum a 20% improvement over the AR1 model).  R^2^ values at the age group level ranged from a low of `r round(min(age_r2$.estimate),2)` in the `r age_r2$age_group[which.min(age_r2$.estimate)]` age group to a high of `r round(max(age_r2$.estimate),2)` in the `r age_r2$age_group[which.max(age_r2$.estimate)]` age group.

```{r age-forecast, fig.cap="Observed (grey ribbons) and predicted (points) numbers of sockeye salmon within each age group returning to Bristol Bay, Alaska. Age group refers to 'years spent in freshwater'_'years spent in ocean'. Color corresponds to the best performing model in terms of scaled root mean squared error (SRMSE), transparency reflects the SRMSE of the best performing model, noted in the top left corner of each panel along with the R^2^ value."}
age_forecast_figure
```

```{r age-returns, fig.cap = "Observed sockeye salmon return abundance by age group (color) to major river systems in Bristol Bay (panels) over time.", eval=FALSE, include=FALSE}

age_system_return_plot
```

## Ensemble Forecasts 


<!-- The result that some models outperform others under specific circumstances (e.g. particular river systems or age classes) suggests a simple solution: pick the best model for the resolution of prediction required, or better yet create an ensemble model that can leverage the relative strengths of individual models into one improved forecast.  -->


In theory the individual models tested here were capable of improving pre-season forecast accuracy over the years 2000-2019 when viewed retrospectively. However, scientists must make annual decisions as to which models to use and how to weigh their predictions. To approximate this process we selected the top performing (in terms of SRMSE) rolling ensemble model (either the FRI or the random forest model-of-models ensemble) for each of the main river systems. In `r comp_ens_sys_improvement$n_improv` of the `r n_distinct(system_forecast$system)` evaluated river systems the random forest ensemble produced the preferred ensemble, improving on the FRI forecast by on average `r percent(comp_ens_sys_improvement$improv_amnt)` ,  with the FRI forecasts being preferable of the two ensembles in the remaining `r n_distinct(system_forecast$system) - comp_ens_sys_improvement$n_improv`  river systems, outperforming the random forest ensemble by `r percent(-comp_ens_sys_improvement$worse_amnt)` .  R^2^ values of the best ensemble model at the river sytem level ranged from a low of `r round(min(ens_r2$.estimate),2)` in the `r ens_r2$system[which.min(ens_r2$.estimate)]` age group to a high of `r round(max(ens_r2$.estimate),2)` in the `r ens_r2$system[which.max(ens_r2$.estimate)]` age group.


```{r ensemble-forecast, fig.cap="Performance of candidate ensemble models. Shape of points indicates which ensemble model had the lowest scaled root mean squared error (SRMSE). FRI refers to the published forecasts by the Fisheries Research Institute. The random forest ensemble is an ensemble model constructed by random forest made out of candidate model forecasts. The forecast from the best performing ensemble is plotted and denoted by point shape. Color of points shows the percent improvement of the ensemble model relative to the published FRI forecast. R^2^ value noted in text in top left corner of each panel."}
system_ensemble_forecast_figure
```

## Frontiers in Performance  

The underlying assumption of such an ensemble strategy is that the information needed for an accurate forecast is present in the data, and the key is finding the combination of individual models that are best able to identify and leverage that information. However, no model can find information that simply is not present, or succeed if it is based upon data that is subject to overwhelming observation or process error.  Examining trends in the annual residuals by model and river system shows clear patterns. In some years and river systems, all models perform similarly well, indicating that the information needed for a good forecast was present and detectable by each of the models (e.g. Nushagak before 2015). In other years, only particular models performed well, while others struggled, indicating that information needed for a robust forecast was present but only some models were able to accurately identify the underlying relationship, highlighting the value of ensemble methods (e.g. Naknek between 2005 and 2010). However, in other years and river systems all models struggled, for example the Wood River in 2018 and the Kvichak River in 2014. This provides evidence that the information needed to generate a robust forecast in those years was simply not present in the data that were available at the time (Fig.\@ref(fig:resids)).

```{r resids, fig.cap="Centered and scaled annual residuals (forecast returns minus observed returns) by river system and model over time. Grey bands indicate areas more than one standard deviation from the mean residuals for a given system. Years in which all the lines are within a grey band indicate periods where all the models struggled to provide reasonable forecasts."}
yearly_system_resid_struggles_figure
```

Our residual analysis suggests that in some instances we simply may need to collect different data for inclusion in the forecast model if we hope to improve forecasts. For example, none of our models were able to predict the massive spike in returns to the Wood river system in recent years (Fig.\@ref(fig:resids)), indicating that a signal of the process resulting in an increase in salmon survival was not among the suite of predictors explored. Conversely, all of the models performed reasonably well over most of the history of the Nushagak, except for the most recent years. This may be explained by the relative lack of variation in historic returns to the Nushagak prior to 2017, allowing both parametric and non-parametric models to perform equally well. 

We can use the results of our most recent estimated boost\_tree model to examine the relative importance of different included data streams in improving forecast skill (Fig.\@ref(fig:imp-plot)). While these importance scores cannot be interpreted in the same manner as regression coefficients, they give us a sense of where we might look for new data to inform prediction. Across all river systems, prior returns in that system were an important predictor (and in many systems past returns in other river systems were also a useful predictor).

```{r imp-plot, eval = TRUE, fig.cap="Mean variable importance across all river systems of variables with importance scores greater than 0.075."}
system_varimportance_figure
```

<!-- Which variables were most important in our forecasts? XX not sure if /how this can be done for the other models, or if it’s worth it XX. Maybe just do variable importance plots by age group for the tree based methods -->

<!-- * Performance in total returns forecast -->

<!-- * Break down by combinations of age class / system -->

<!-- * Variable importance where available -->

<!-- * Synthesize top performs for different objectives  -->

<!-- * Result of simple ensemble -->

<!-- * Result of models predicting boom / bust years  -->

# Discussion 

While our tested methods made meaningful improvements in forecast accuracy in many cases, no one model type stood out as a clear winner, highlighting the need for multi-model inference in ecological forecasting. Viewed in retrospect individual models tested here were able to make substantial improvements in forecast accuracy (Fig.\@ref(fig:sys-forecast)-\@ref(fig:age-forecast)). However, the best retrospective model over the years 2000-2020 varied widely by age group and system, presenting a challenge for decision makers charged with picking which model to use for a particular forecast. Ensemble models such as the random forest ensemble (i.e. a "model-of-models") constructed here can help users separate out the signal from the noise in historic model performance, which in this case resulted in modest improvements in forecast still in the majority of river systems evaluated in this study. 

Our results cannot be interpreted as a generalized assessment of the relative strengths or weaknesses of the types of models evaluated here. Model performance is a complex function of the suitability of a given model for the task at hand, the data made available to it, and a wide range of design decisions. This is reflected in the diversity of models classified as the best performer depending on the specific question being asked of them in our analysis. Our claim is not that boosted regression trees for example are inherently best at predicting the dynamics of the Wood river system, but rather that under these particular conditions boosted regression tree happened to work best. Attempts to classify more general "best" models for salmon forecasting would require considering a broader range of empirical and simulated states, as well as increased standardization of the design decision process. Similarly, the relative performance of the random forest ensemble model to the historic FRI forecast cannot be construed as a  broader result about the relative performance of "statistical" vs. "manual" models. The random forest ensemble utilized a range of models and data sets there were not all available to the historic FRI forecasts, and it is entirely possible that had the FRI had access to those same models in the past they would have produced more similar results to the random forest ensemble. What our results do show is that addition of new model types and data does provide potential to improve on the historic FRI methods such as they were. 

<!-- The clearest outcome from our results at the river system level is that retrospectively no one model performed better than all others, and river systems varied greatly in their predictability. Why is it that some models outperform others at the river system level? One explanation is that simply, given a set of roughly equally effective models and a short period of cross-validation (n = 19 years), we would expect different models to perform best in different river systems by chance alone in some cases. The mean difference in SRMSE between the best and second-best models by river system was only `r prettyNum(mean_srmse_delta$mmd, digits = 1)`, demonstrating that several models are capable of outperforming a lag(1) model with roughly equal skill in many cases. -->

<!-- Another explanation could relate to the age group makeup of runs in different river systems. While it is not intuitive as to why one model would perform better than another in a given river system all else being equal, we might expect different models to perform better for different age classes. The dynamic linear model is at its core still a sibling regression, and as such we would expect it to perform better on older age groups for which there is more reliable information on survival from returns of younger age classes. Conversely, while the machine learning and EDM models can use sibling returns, they are more flexible in determining which data are most useful for prediction. As such, predictions for younger age classes can be based more on environmental signals if called for by the data. The boosted regression tree model, which allows for environmental and interspecies effects on salmon returns, performed best for the youngest included age group (1.2 fish) (Fig.\@ref(fig:age-forecast)). Turning back to the river systems, returns to Naknek, where boost\_tree performed best, have recently had large proportions of their runs made up of 1.2 fish (the group for which boost\_tree performed best) (Fig.\@ref(fig:age-returns)). However, the Wood river system has also seen a massive increase in 1.2 fish, and no model was able to correctly capture the recent returns to that river system. -->


Using multiple types of highly flexible parametric and non-parametric models can provide insight into whether historic limits to forecast skill were likely due to limitations in the information content of the available data, or from simply not finding the best model to apply to the data at hand. While we were able to improve forecast skill of Bristol Bay sockeye salmon in some instances, in particular years and systems all tested models performed poorly. These events may reflect changes in the effect of currently observed data, i.e. a violation of the assumption that the past correlation between a variable and salmon returns will apply in the future, or may be indicative of the underling effect of an unobserved variable. The former case may be resolved by simply giving the model more years on which to train, or through explicit techniques for modeling outlier events in the manner of @anderson2017c. The latter case can only be resolved through the inclusion of new data that contains information on the previously omitted process. 

<!-- annual return of sockeye salmon to the river systems of Bristol Bay Alaska is an economic and ecological marvel, providing critical nutrients and value to Alaska's ecosystems, economy, and people. Efficient and sustainable use of this critical natural and cultural resource depends in part on the ability to accurately forecast the number of salmon that will return each year, prior to the fishing season. As such, accurate and precise forecasts for future salmon abundance are vitally important to the biological and economic sustainability of this important fishery. Allowing environmental and cross-stock correlations to dynamically inform the predictions of sockeye salmon through modern predictive models produced meaningful improvements in our ability to forecast annual returns of these iconic fish. -->

That forecasts for individual river systems can be improved by treating historic returns in other river systems as predictors, as evidenced by the machine learning models, is an important finding. The historically used largely parametric salmon forecast methods have largely focused on relationships among age classes within single river systems in isolation. While perhaps not surprising given the juvenile salmon from multiple river systems enter the same area of the eastern Bering Sea during approximately the same season and likely experience similar survival conditions at ocean entry, this result suggests that sharing age-specific return abundance information among salmon stocks and river systems within Bristol Bay can inform and improve predictive performance.

In addition to the return abundance of salmon from the home and neighboring river systems, we found that oceanographic variables including mean sea surface temperature and sea surface air pressure throughout the spatial and temporal range of the oceanic phase of these salmon were informative predictors for some river systems. In addition, as reported by @connors2020, in some instances the abundance of other salmon species (chum salmon, *Oncorhynchus keta*, in western Kamchatka and northern British Columbia, pink salmon, *Oncorhynchus gorbuscha*, in Prince William sound) proved important predictors of Bristol Bay salmon return abundance (Fig.\@ref(fig:imp-plot)). Going forward, data on freshwater conditions, interspecies competitors, and the size structure of the salmon populations may prove useful in improving forecast accuracy. 

Takens' theorem suggests that the dynamics of a variable, in this case salmon returns, can be reconstructed simply by the lags of that variable, potentially obviating the need to collect the right covariates in order to provide an accurate forecast [@munch2020]. While this may be true given sufficient sample size, temporal coverage, and lack of observation error, all of these conditions rarely hold in ecological forecasting. Case in point, the EDM models tested here were unable to accurately predict many events in the Bristol Bay sockeye return history (Fig.\@ref(fig:resids)), indicating that the attractor constructed out of the lagged returns alone did not have the right information needed to forecast particular events. In these cases, collection and use of relevant covariates may help the model make improved predictions than are possible given lagged returns alone.  

Traditional pre-season forecast methods for sockeye salmon returning to Bristol Bay and throughout Alaska have often assumed that relationships among age classes are static over time. However, there is increasing recognition of time-varying relationships between Alaskan salmon production and sea surface temperature [@litzow2018], and large-scale oceanographic processes including the Pacific Decadal Oscillation [@litzow2020;@litzow2020a]. Given evidence for the dynamic nature of salmon-climate relationships, it should not be surprising that salmon abundance forecast relationships should also exhibit temporal variability. While not informed by environmental data and only leveraging information from a single river system, the DLM approach was found to exhibit superior performance in several river systems and the 2.2 age class. It seems reasonable that the flexible nature of the DLM approach to capture time-varying dynamics in both average abundance and the ratio among age classes permits an indirect accounting for the dynamic salmon-environment processes that are increasingly recognized. 

Forecast methods historically employed by the Fisheries Research Institute involved evaluation of a suite of alternative forecast models in each year, and selection of a preferred model and data time series on which to train the model (i.e. 1963 onward or after the observed shift in the Pacific Decadal Oscillation in 1980), for each salmon stock by age class combination based on forecast bias and precision over the recent 20-year period. While the FRI forecast has always been primarily based on the relationship between the abundance of age classes from the same cohort among successive years, the suite of forecast models explored as part of the FRI forecast has evolved over time. In recent years new methods have been added to the forecast model suite including autoregressive integrated moving average (ARIMA) models, boosted regression trees, Bayesian indicator variable methods, and dynamic linear models. The manual model selection process at the heart of the FRI ensemble approach has proven effective over time at identifying candidate models for forecast groups (stock-by-age) that best leverage patterns within individual time series (i.e. ARIMA, DLM), weighting candidate predictor age classes (i.e. Bayesian indicator variable methods), and non-linear relationships between the return abundance of age classes for a stock in prior years (i.e. boosted regression trees). However, despite the observed value in comparing performance of alternative forecast model types inherent in the FRI forecasting approach, significant forecast errors have occurred. The range of models historically used by the FRI only leveraged data for sibling age classes from the same river system, and the potential for human error in the manual model selection process cannot be overlooked. We demonstrate here that statistical ensemble approaches, such as the random forest ensemble, present a viable complement to more "human based" ensemble approaches. 

We demonstrate here how parametric and non-parametric modeling approaches can provide improvements in ecological forecasting. @ward2014 also explored the use of models similar to those used here in the context of ecological forecasting of time-series data from natural populations. They however found that the sorts of tools explored here generally performed worse than simple autoregressive models, while being substantially more computationally intensive. In contrast we found that our lag(1) benchmark model was outperformed or equaled by one or more of our models across nearly every resolution we evaluated. What might explain this difference? First, @ward2014 specifically designed their study around making predictions of future population size solely based on historic population size, while the forecast methods we explore here were informed by the abundance of multiple salmon age classes or stocks, and in some cases by environmental conditions and the abundance of other salmon species. Second, @ward2014 did find that more complex models such as random forests and neural networks performed well for some salmon populations, particularly those characterized by regular cyclic behavior. Our results are broadly consistent then with the findings of @ward2014 in this respect.

However, @ward2014 did find that more complex models performed poorly relative to their baseline random walk model in salmon stocks that exhibited less cyclic behavior. In contrast, we found near universal improvements over our baseline model to some degree across all river systems and age groups, including those that either do not seem to exhibit cyclic behavior, or have experienced a break from past cycles in recent years (Fig.\@ref(fig:returns), Fig.S9). The machine learning methods explored here have access to much more data than the historic returns alone though, including environmental conditions and abundance of other salmonids. In addition, the machine learning methods used here were able to leverage correlations in returns across multiple age groups and river systems (Fig.\@ref(fig:imp-plot)). While we have access to over 50 years of data, longer than some of the series reported in @ward2014, our sample sizes are still minute compared to the sample sizes in most applications of machine learning methods, indicating that these methods can still be used with the relatively small sample sizes often encountered in forecasting the population dynamics of harvested species. These differences of long timeseries, use of cross system and age group correlations, and inclusion of environmental covariates may explain the ability of the more complex models tested here to outperform benchmark lag(1) models even in systems without an obvious cyclical pattern. 

One of the primary advantages of parametric statistical approaches that make explicit assumptions about data generating processes, and by extension error structure, is that they provide estimates of the degree of uncertainty associated with a model coefficient or a prediction. Non-parametric machine learning methods are powerful in that they are able to learn complex predictive correlations within data, but a key limitation of these methods is that they generally do not provide estimates of uncertainty for their predictions. We cannot therefore provide 95% confidence intervals or other conventional metrics of uncertainty around many of our forecasts, though the SRMSE values provide an estimate of the historic error in the forecasts of each model. The distribution of Pearson's residuals for each of the models do not exhibit any clear differences across models (see SI). 




<!--   potential for these more complex models to improve salmon  -->


<!-- We would only expect the types of models employed here to provide an improvement over simpler methods if there are substantial complexities in the relationship between past and future abundance that simpler models miss (i.e. the simpler models are misspecified) -->

<!-- The results of @ward2014 suggest simpler models may indeed be preferable when only historic timeseries of individual stocks are available as a data source, and additional informative predictors (such as environmental data or nearby related systems) are not available. Similarly, the models used here are data intensive, and our current application is for the Bristol Bay sockeye salmon system which stands as an outlier in ecology in terms of both the quality and duration (over 50 years) of population-level data available on which to base predictions. More limited time series of reconstructed return abundance are often available in other regions and for other salmon species. It is possible that when constrained to forecasting salmon abundance for stocks with limited numbers of annual observations for training, testing and tuning, simpler models may indeed provide more robust prediction. -->


<!-- We find though that these potentially more complex parametric and non-parametric models can provide meaningful improvements when given additional covariates and/or structure. -->

<!-- What other data should we suggest? We don’t include any in-river condition data, do we just not have it? Thoughts on other places we can invest? [g] -->

# Conclusion  

The field of ecology is generally concerned with developing theories and evidence for why ecosystems are structured and behave the ways they do. This pursuit of heuristic understanding can lead to construction of interpretable models that provide insight about system dynamics, but limited predictive power. However, for specific application in areas such as pre-season salmon abundance forecasts the objective is solely to obtain accurate and precise predictions one year into the future. We designed and optimized our models solely around predictive power, and while some methods such as empirical dynamic modeling and dynamic linear models can provide both insight and predictive skill, the machine learning methods tested here (boosted regression tree and random forests) are focused on prediction alone, with limited scope to improve ecological insight. In the case of natural resources management that often depends on making decisions today based on predictions about the future, prediction-focused methods such as those presented here can present substantial opportunity. Here we show that incorporating multiple predictive models into a statistical ensemble was able to provide some meaningful improvements in the pre-season forecast accuracy of Bristol Bay sockeye salmon. 

<!-- In particular, leveraging multiple different modeling approaches can help uncover otherwise hidden information in available data. As we demonstrate here though, we cannot simply rely on computer age statistical methods to automatically solve problems of prediction in ecology. Even when groups of statistical methods are unable to provide accurate forecasts, using multiple computer age methods as presented here can help identify when these prediction failures are likely due to model misspecification, and when they are do to a simple lack of sufficiently informative data. Computer-age statistical methods then can both improve forecasts and help identify frontiers in predictability in ecological systems given currently available data. Once these borders are found, we can focus efforts and resources on collecting new data that can push our predictive models to greater heights.  -->

<!-- Given the flexible nature and varying assumptions of our evaluated models, these isolated collective forecast failures help identify when forecast performance is limited by model specification versus the constraints imposed by the nature and quality of available data. Our results demonstrate the utility and limitations of modern computational tools for applied management of natural resources, even when confronted with the short time series and observation error common in ecological systems. -->

Accurate forecasts are a crucial part of natural resource management, a task made increasingly challenging by climate change. Our gains in forecast accuracy for the economically and ecologically critical Bristol Bay sockeye salmon fishery demonstrate the ability of parametric and non-parametric models to make meaningful improvements in short-term predictive ability for the abundance of natural populations faced with a rapidly changing environment. By combining multiple models types we are able to identify likely frontiers in forecast performance given currently available data. However, even for this relatively robust dataset we were fundamentally unable to predict the returns of particular river systems and age classes in certain years. The collective failure of multiple methods in specific time steps and locations helps clarify instances in which the only likely path to meaningful forecast improvement is collection of additional data, while also highlighting the potentially irreducible impact of observation error on the limits of forecast performance. It is critical that we allocate resources to both the advancement of predictive modeling methods in ecology, and to the hard work of collecting the data from the natural world that are the foundation of any successful forecasting efforts.


# Author contribution statement {.unnumbered}

D.O., C.C., and P.K. conducted the analyses. All authors contributed to the development of the manuscript.

# Funding statement {.unnumbered}

Funding for this study was provided by the Bristol Bay Regional Seafood Development Association, by the Bristol Bay Seafood Processors, and by Douglas and Joyce McCallum.

# Data availability statement {.unnumbered}

All data, code, and package dependencies needed to fully reproduce our results are publicly available at www.github.com/danovando/salmon-forecast-paper.

<!-- # Competing Interests -->

<!-- R.H's research program receives funding from environmental NGOs, foundations, fishing industry, governments and international agencies. All of these can be interpreted as a conflict of interest when evaluating fisheries policy. -->

<!-- # Materials & Correspondence. -->

<!-- Materials can be accessed at www.github.com/danovando/salmon-forecast-paper -->

# References

::: {#refs}
:::

